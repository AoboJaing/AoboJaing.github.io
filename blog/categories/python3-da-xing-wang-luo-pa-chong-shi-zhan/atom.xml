<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Python3 大型网络爬虫实战 | AoboSir 博客]]></title>
  <link href="http://aobojaing.github.io/blog/categories/python3-da-xing-wang-luo-pa-chong-shi-zhan/atom.xml" rel="self"/>
  <link href="http://aobojaing.github.io/"/>
  <updated>2016-12-26T00:37:42+08:00</updated>
  <id>http://aobojaing.github.io/</id>
  <author>
    <name><![CDATA[Aobo Jaing]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Python3 大型网络爬虫实战 003 — Scrapy 大型静态图片网站爬虫项目实战 — 实战：爬取 169美女图片网 高清图片]]></title>
    <link href="http://aobojaing.github.io/blog/2016/12/26/python3-large-web-crawler-169bb-com-HD-beautiful-pictures/"/>
    <updated>2016-12-26T00:14:57+08:00</updated>
    <id>http://aobojaing.github.io/blog/2016/12/26/python3-large-web-crawler-169bb-com-HD-beautiful-pictures</id>
    <content type="html"><![CDATA[<hr />

<p>[TOC]</p>

<h2>开发环境</h2>

<ul>
<li>Python第三方库：lxml、Twisted、pywin32、scrapy</li>
<li>Python 版本：python-3.5.0-amd64</li>
<li>PyCharm软件版本：pycharm-professional-2016.1.4</li>
<li>电脑系统：Windows 10 64位</li>
</ul>


<p>如果你还没有搭建好开发环境，请到<a href="http://www.aobosir.com/blog/2016/11/26/python3-large-web-crawler-001-Build-development-environment/">这篇博客</a>。</p>

<hr />

<ul>
<li>本篇博客源代码GitHub里：<a href="https://github.com/AoboJaing/secondDemo/">这里</a>。</li>
</ul>


<p>这一篇博客的目的是爬取 <a href="http://www.169bb.com/">169美女图片网</a> 里面的所有的“<a href="http://www.169bb.com/xiyangmeinv/">西洋美女</a>”的高清图片。</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480187579013.png" alt="Alt text" /></p>

<hr />

<p>爬虫程序设计思路：</p>

<p>1 . 先得到 <a href="http://www.169bb.com/xiyangmeinv/">http://www.169bb.com/xiyangmeinv/</a> 页面里面所有的照片后面对应的URL网页链接（<a href="http://www.169bb.com/xiyangmeinv/2016/1123/37380.html">如</a>）。</p>

<p>2 . 接着在得到的URL链接网页里面得到里面所有高清图片的下载地址，进行下载。</p>

<p>3 . 得到所有 “西洋美女” 网页的页数。</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480187965472.png" alt="Alt text" /></p>

<hr />

<h2>观察网页 和 网页源代码</h2>

<p>1 . 打开 169美女图片网：<a href="http://www.169bb.com/">http://www.169bb.com/</a></p>

<p>2 . 我们的目的是爬取这个站点里面所有 “西洋美女” 的高清图片。所以点击进入“西洋美女” 标签里。（<a href="http://www.169bb.com/xiyangmeinv/%EF%BC%89">http://www.169bb.com/xiyangmeinv/%EF%BC%89</a></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480188171424.png" alt="Alt text" /></p>

<p>3 . 观察这个页面，在页面最下面，显示了，当前一共311页。</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480188308560.png" alt="Alt text" /></p>

<p>4 . 我们再来观察每页的网址有什么共同点，我们发现：</p>

<ul>
<li>第2页的网站是：<a href="http://www.169bb.com/xiyangmeinv/list_4_2.html">http://www.169bb.com/xiyangmeinv/list_4_2.html</a></li>
<li>第3页的网址是：<a href="http://www.169bb.com/xiyangmeinv/list_4_3.html">http://www.169bb.com/xiyangmeinv/list_4_3.html</a></li>
<li>第1页的网站是：<a href="http://www.169bb.com/xiyangmeinv/list_4_1.html">http://www.169bb.com/xiyangmeinv/list_4_1.html</a></li>
</ul>


<p>这样每页的网址是有规律的，按照这个规律，我们可以推测出“西洋美女” 的第120页的网址就应该是：<a href="http://www.169bb.com/xiyangmeinv/list_4_120.html">http://www.169bb.com/xiyangmeinv/list_4_120.html</a>
。事实的确是这样的。好。</p>

<p>5 . 现在，我们随便点击一个图片，进去看看这个美女的高清图片集。</p>

<p>里面都是高清的图片，并且有很多，并且，不止一页。就我随机点击的这个美女的链接就有11页，并且一页里面有5张左右的高清图片。</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480188754820.png" alt="Alt text" /></p>

<p>6 . 并且它的每页的网址也是有规律的。</p>

<ul>
<li>第2页是：<a href="http://www.169bb.com/xiyangmeinv/2016/1117/37333_2.html">http://www.169bb.com/xiyangmeinv/2016/1117/37333_2.html</a></li>
<li>第3页是：<a href="http://www.169bb.com/xiyangmeinv/2016/1117/37333_3.html">http://www.169bb.com/xiyangmeinv/2016/1117/37333_3.html</a></li>
<li>第1页是：<a href="http://www.169bb.com/xiyangmeinv/2016/1117/37333.html">http://www.169bb.com/xiyangmeinv/2016/1117/37333.html</a></li>
<li>&hellip;</li>
</ul>


<p>但是有的美女的网址里只有一页，比如这个：<a href="http://www.169bb.com/xiyangmeinv/2016/0103/5974.html">http://www.169bb.com/xiyangmeinv/2016/0103/5974.html</a></p>

<hr />

<p>好了，现在这个目标网页，我们已经分析完了。现在就可以编程。</p>

<hr />

<h2>写程序</h2>

<p>源代码GitHub里：<a href="https://github.com/AoboJaing/secondDemo/">这里</a>。</p>

<p>接下来我们为大家讲解大型图片爬虫项目编写实战。</p>

<p><strong>Step 1 . </strong></p>

<p>创建一个Scrapy爬虫项目：</p>

<pre><code>scrapy startproject secondDemo
</code></pre>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480190424967.png" alt="Alt text" /></p>

<p>创建一个scrapy爬虫文件，我们就在这个文件里面写爬取目标网站里面图片的爬虫程序。</p>

<pre><code>cd secondDemo
scrapy genspider -t basic pic_169bb 169bb.com
</code></pre>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480190488955.png" alt="Alt text" /></p>

<hr />

<p>用 <strong>PyCharm</strong> 软件打开刚刚创建的 <code>secondDemo</code> 工程。</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480190609159.png" alt="Alt text" /></p>

<p><strong>Step 2 . </strong> 在 <code>items.py</code> 文件里面的<code>SeconddemoItem()</code>函数里面创建一个对象，这个对象在其他的文件里面会使用到。</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480191956323.png" alt="Alt text" /></p>

<p><strong>Step 3 . </strong> 现在开始爬虫的编写。进入<code>pic_169bb.py</code>文件。</p>

<p>爬虫（<code>pic_169bb.py</code>文件）会自动的先爬首页（<a href="http://169bb.com/">169bb.com</a>），爬完首页之后，会自动的进入<code>parse()</code>回调函数。</p>

<p>这个回调函数中，我们需要写些东西。</p>

<p>先获取所有栏目的名字和地址。</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480192407198.png" alt="Alt text" /></p>

<p>查看源代码：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480192613372.png" alt="Alt text" /></p>

<p>在<code>pic_169bb.py</code> 文件中的 <code>parse()</code>回调函数中添加下面的代码：</p>

<pre><code class="python">        urldata = response.xpath("/html/body/div[@class='header']/div[@class='hd_nav']/div[@class='w1000']//a/@href").extract()
        print(urldata)
</code></pre>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480196844577.png" alt="Alt text" /></p>

<p>现在运行一下，输出：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480196997090.png" alt="Alt text" /></p>

<hr />

<p>继续进行下一次的爬取：</p>

<p>先导入一个模块</p>

<pre><code class="python">from scrapy.http import Request
</code></pre>

<hr />

<p>爬取子栏目的第一页，即西洋美女网址的第一页。</p>

<pre><code class="python">        xiyangurldata = urldata[4]  # 获取西洋美女首页网址
        print(xiyangurldata)
        yield Request(url=xiyangurldata, callback=self.next)

    def next(self, response):
        pass
</code></pre>

<hr />

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480198132160.png" alt="Alt text" /></p>

<pre><code class="python">    def next(self, response):
        page_title_list = response.xpath("/html/body//div[@class='w1000 box03']/ul[@class='product01']//li/a/@alt").extract()
        print(page_title_list)
        page_url_list = response.xpath("/html/body//div[@class='w1000 box03']/ul[@class='product01']//li/a/@href").extract()
        print(page_url_list)
        pass
</code></pre>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480200182166.png" alt="Alt text" /></p>

<p>运行输出一下：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480200281135.png" alt="Alt text" /></p>

<hr />

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480200630246.png" alt="Alt text" /></p>

<pre><code class="python">        page_num = response.xpath("//span[@class='pageinfo']//strong/text()").extract()[0] # 得到西洋美女总页数
        print(page_num)
        print(response.url)
        for i in range(1, int(page_num)+1):
            page_url = response.url + 'list_4_'+ str(i) + '.html' # 得到西洋美女每一个页面的网址
            print(page_url)
        pass
</code></pre>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480201913887.png" alt="Alt text" /></p>

<p>运行输出一下：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480201965248.png" alt="Alt text" /></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480201944373.png" alt="Alt text" /></p>

<hr />

<p>继续下一次：</p>

<pre><code class="python">        for i in range(1, int(page_num)+1):
            page_url = response.url + 'list_4_'+ str(i) + '.html' # 得到西洋美女每一个页面的网址
            print(page_url)
            yield Request(url=page_url, callback=self.next2)
        pass

    def next2(self, response):
        pass
</code></pre>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480202151141.png" alt="Alt text" /></p>

<hr />

<p>现在获取每一个美女的网页网址：</p>

<pre><code class="python">    def next2(self, response):
        page_title_list = response.xpath("/html/body//div[@class='w1000 box03']/ul[@class='product01']//li/a/@alt").extract()
        # print(page_title_list)
        page_url_list = response.xpath("/html/body//div[@class='w1000 box03']/ul[@class='product01']//li/a/@href").extract()
        # print(page_url_list)

        for i in range(0, len(page_url_list)):
            gril_page_url = page_url_list[i]
            print(gril_page_url)
            yield Request(url=gril_page_url, callback=self.next3)
        pass

    def next3(self, response):
        pass
</code></pre>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480203708834.png" alt="Alt text" /></p>

<p>运行程序看看：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480203763527.png" alt="Alt text" /></p>

<hr />

<p><code>next3()</code> 这个回调函数的功能就是得到一个美女网页里面的所有的页面的网址。</p>

<p>有的美女的网页里面只有一个页面，有的美女的网页里面有多个页面：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1482656828793.png" alt="Alt text" /></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1482656883815.png" alt="Alt text" /></p>

<p>可以统一解决。</p>

<hr />

<p>测试：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480204459567.png" alt="Alt text" /></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480204419920.png" alt="Alt text" /></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480205200021.png" alt="Alt text" /></p>

<p>输出：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480205472599.png" alt="Alt text" /></p>

<hr />

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1482656683950.png" alt="Alt text" /></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480205432817.png" alt="Alt text" /></p>

<p>同样的回调函数</p>

<p>输出：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480205458465.png" alt="Alt text" /></p>

<hr />

<p>所以，我们可以这样写程序：</p>

<p>当得到的页码为-3，说明这个美女的网页是单页的；如果得到的页码数不等于-3，说明这个美女的网页是多也的。</p>

<p>测试程序：</p>

<p>对于多页面的美女网页网址</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480229614160.png" alt="Alt text" /></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480229673684.png" alt="Alt text" /></p>

<p>运行输出：</p>

<pre><code>D:\WorkSpace\python_ws\python-large-web-crawler\secondDemo&gt;scrapy crawl pic_169bb --nolog
10
http://www.169bb.com/xiyangmeinv/2016/0717/36463.html
http://www.169bb.com/xiyangmeinv/2016/0717/36463_5.html
http://www.169bb.com/xiyangmeinv/2016/0717/36463_10.html
http://www.169bb.com/xiyangmeinv/2016/0717/36463_9.html
http://www.169bb.com/xiyangmeinv/2016/0717/36463_6.html
http://www.169bb.com/xiyangmeinv/2016/0717/36463_8.html
http://www.169bb.com/xiyangmeinv/2016/0717/36463_7.html
http://www.169bb.com/xiyangmeinv/2016/0717/36463_3.html
http://www.169bb.com/xiyangmeinv/2016/0717/36463_4.html
http://www.169bb.com/xiyangmeinv/2016/0717/36463_2.html

D:\WorkSpace\python_ws\python-large-web-crawler\secondDemo&gt;
</code></pre>

<p>对于单页面的美女网页：</p>

<pre><code class="python">    def parse(self, response):
        ...
        yield Request(url='http://www.169bb.com/xiyangmeinv/2016/0103/2268.html', callback=self.demo)
</code></pre>

<p>回调函输一样。</p>

<p>运行输出：</p>

<pre><code class="python">D:\WorkSpace\python_ws\python-large-web-crawler\secondDemo&gt;scrapy crawl pic_169bb --nolog
-3
http://www.169bb.com/xiyangmeinv/2016/0103/2268.html

D:\WorkSpace\python_ws\python-large-web-crawler\secondDemo&gt;
</code></pre>

<p>成功。</p>

<p>所以现在的爬虫代码应该是这样的：</p>

<pre><code class="python"># -*- coding: utf-8 -*-
import scrapy
from scrapy.http import Request

class Pic169bbSpider(scrapy.Spider):
    name = "pic_169bb"
    allowed_domains = ["169bb.com"]
    start_urls = ['http://169bb.com/']

    def parse(self, response):
        title_list = response.xpath("/html/body/div[@class='header']/div[@class='hd_nav']/div[@class='w1000']//a/text()").extract()
        # print(title_list)
        urldata = response.xpath("/html/body/div[@class='header']/div[@class='hd_nav']/div[@class='w1000']//a/@href").extract()
        #print(urldata)
        xiyang_title = title_list[4] # 获取西洋美女标签的文本内容
        xiyang_urldata = urldata[4]  # 获取西洋美女首页网址
        # print(xiyang_title, xiyang_urldata)
        yield Request(url=xiyang_urldata, callback=self.next)
        # yield Request(url='http://www.169bb.com/xiyangmeinv/2016/0717/36463.html', callback=self.demo)
        # yield Request(url='http://www.169bb.com/xiyangmeinv/2016/0103/2268.html', callback=self.demo)

    def next(self, response):
        page_num_str = response.xpath("//span[@class='pageinfo']//strong/text()").extract()[0] # 得到西洋美女总页数
        # print(page_num_str)
        # print(response.url)
        for i in range(1, int(page_num_str)+1):
            page_url = response.url + 'list_4_'+ str(i) + '.html' # 得到西洋美女每一个页面的网址
            # print(page_url)
            yield Request(url=page_url, callback=self.next2)
        pass

    def next2(self, response):
        page_title_list = response.xpath("/html/body//div[@class='w1000 box03']/ul[@class='product01']//li/a/@alt").extract()
        # print(page_title_list)
        page_url_list = response.xpath("/html/body//div[@class='w1000 box03']/ul[@class='product01']//li/a/@href").extract()
        # print(page_url_list)

        for i in range(0, len(page_url_list)):
            gril_page_url = page_url_list[i] # 得到西洋美女页面里面每一个美女的网页网址
            print(gril_page_url)
            yield Request(url=gril_page_url, callback=self.next3)
        pass

    def next3(self, response):
        rela_pages_list = response.xpath("//div[@class='dede_pages']/ul//li/a/text()").extract()
        pages_num = len(rela_pages_list) - 3
        # print(pages_num)
        self.getPic(response)
        if pages_num == -3:
            # pages_num = 1
            return
        for i in range(2, pages_num+1):
            girl_page_url = response.url.replace('.html', '_') + str(i) + '.html'
            # print(girl_page_url)
            yield Request(url=girl_page_url, callback=self.next4)
        pass

    # def demo(self, response):
    # #     rela_pages_list = response.xpath("//div[@class='dede_pages']/ul//li/a/text()").extract()
    # #     pages_num = len(rela_pages_list)-3
    # #     print(pages_num)
    # #     pass
    #     rela_pages_list = response.xpath("//div[@class='dede_pages']/ul//li/a/text()").extract()
    #     pages_num = len(rela_pages_list) - 3
    #     # print(pages_num)
    #     self.getPic(response)
    #     if pages_num == -3:
    #         # pages_num = 1
    #         return
    #     for i in range(2, pages_num+1):
    #         girl_page_url = response.url.replace('.html', '_') + str(i) + '.html'
    #         # print(girl_page_url)
    #         yield Request(url=girl_page_url, callback=self.next4)
    #     pass

    def next4(self, response):
        self.getPic(response)
        pass

    def getPic(self, response):
        print(response.url)
        pass
</code></pre>

<hr />

<p>现在，我们需要在<code>getPic()</code> 函数中获取每一个美女网页的每一个页面里面的所有高清图片。</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480231492634.png" alt="Alt text" /></p>

<pre><code class="python">    def getPic(self, response):
        # print(response.url)
        item = SeconddemoItem()
        item['url'] = response.xpath("//div[@class='big-pic']/div[@class='big_img']//p/img/@src").extract()
        print(item['url'])
        pass
</code></pre>

<p>测试运行：</p>

<p>测试多页的美女网页：</p>

<p>先将<code>parse()</code>函数的最后一行改为：<code>yield Request(url='http://www.169bb.com/xiyangmeinv/2016/0717/36463.html', callback=self.demo)</code></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480230996305.png" alt="Alt text" /></p>

<p>测试单页的美女网页：</p>

<p>将<code>parse()</code>函数的最后一行改为：<code>yield Request(url='http://www.169bb.com/xiyangmeinv/2016/0103/2268.html', callback=self.demo)</code></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480237427282.png" alt="Alt text" /></p>

<p>成功。</p>

<hr />

<h3>下载高清图片</h3>

<p>OK，现在我们就已经得到了所有西洋美女的所有高清图片的下载地址，我们在<code>piplines.py</code> 文件中使用它们。</p>

<p>删除了<code>next4()</code>回调函数。在<code>demo()</code>回调函数里面调用的都是<code>getPic()</code>回调函数。</p>

<pre><code class="python">    def demo(self, response):
    #     rela_pages_list = response.xpath("//div[@class='dede_pages']/ul//li/a/text()").extract()
    #     pages_num = len(rela_pages_list)-3
    #     print(pages_num)
    #     pass
        rela_pages_list = response.xpath("//div[@class='dede_pages']/ul//li/a/text()").extract()
        pages_num = len(rela_pages_list) - 3
        # print(pages_num)
        self.getPic(response)
        if pages_num == -3:
            # pages_num = 1
            return
        for i in range(2, pages_num+1):
            girl_page_url = response.url.replace('.html', '_') + str(i) + '.html'
            # print(girl_page_url)
            yield Request(url=girl_page_url, callback=self.getPic)
        pass

    # error : yield 经过了一个中间函数，运行就有问题。我现在还不知道为什么
    # def next4(self, response):
    #     self.getPic(response)
    #     pass
</code></pre>

<p>并将<code>getPic()</code>函数里面的item写到生成器里面：</p>

<pre><code class="python">    def getPic(self, response):
        # print(response.url)
        item = SeconddemoItem()
        item['url'] = response.xpath("//div[@class='big-pic']/div[@class='big_img']//p/img/@src").extract()
        # print(item['url'])
        # pass
        yield item
</code></pre>

<p>测试：</p>

<p>测试单页的美女网页：</p>

<p>将<code>parse()</code>函数的最后一行改为：<code>yield Request(url='http://www.169bb.com/xiyangmeinv/2016/0103/2268.html', callback=self.getPic)</code></p>

<p>成功。</p>

<p>测试多页的美女网页：</p>

<p>先将<code>parse()</code>函数的最后一行改为：<code>yield Request(url='http://www.169bb.com/xiyangmeinv/2016/0717/36463.html', callback=self.demo)</code></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480230996305.png" alt="Alt text" /></p>

<p>也算算成功，因为有重复的文件名，所以自动替换，所以这里需要做修改。</p>

<p>我们可以使用美女图片网址里面的数字作为图片文件名的固定前缀来给图片命名，使用正则表达式获取网址的数字。</p>

<hr />

<p>在 <code>pipelines.py</code> 文件 中的 <code>process_item()</code> 函数中使用正则表达式得到图片下载网址的数字：</p>

<pre><code class="python">import re
import urllib.request

class SeconddemoPipeline(object):
    def process_item(self, item, spider):
        # print(len(item['url']))
        for i in range(0, len(item['url'])):
            this_url = item['url'][i]
            id = re.findall('http://724.169pp.net/169mm/(.*?).jpg', this_url)[0]
            id = id.replace('/', '_')
            print(id)
            # file = 'D:/WorkSpace/python_ws/python-large-web-crawler/xiyangmeinv/' + str(i) + '.jpg'
            # print('Downloading :' , file)
            # urllib.request.urlretrieve(this_url, filename=file)
            # print('Final Download :' , file)
        return item
</code></pre>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480241683726.png" alt="Alt text" /></p>

<p>要想使用 <code>pipelines.py</code> 文件 中的 <code>SeconddemoPipeline</code> 类，需要在 <code>settings.py</code> 文件里面设置 <code>ITEM_PIPELINES</code> 项：</p>

<pre><code class="python">ITEM_PIPELINES = {
   'secondDemo.pipelines.SeconddemoPipeline': 300,
}
</code></pre>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480242336499.png" alt="Alt text" /></p>

<blockquote><p>这里，我有一件事情不懂，关于正则表达式的： 网址里面的<code>.</code> 也是正则表达式中的工具字符，也是数据中中的内容，那么正则表达式是如何分辨它在这里是功能字符还是内容字符？</p></blockquote>

<p>在<code>pic_169bb.py</code> 文件的<code>demo()</code> 回调函数中，这样写才能获取到美女网页的第一页的图片地址：</p>

<pre><code class="python">        # error
        # self.getPic(response)
        # succes 为啥将下面的代码用self.getPic(response)的形式不能正常的获取到，而使用下面的代码却能获取到？
        item = SeconddemoItem()
        item['url'] = response.xpath("//div[@class='big-pic']/div[@class='big_img']//p/img/@src").extract()
        # print(item['url'])
        # pass
        yield item
</code></pre>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480241496168.png" alt="Alt text" /></p>

<p>运行程序试试：</p>

<p>测试单页的美女网页：</p>

<p>将<code>parse()</code>函数的最后一行改为：<code>yield Request(url='http://www.169bb.com/xiyangmeinv/2016/0103/2268.html', callback=self.getPic)</code></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480241922348.png" alt="Alt text" /></p>

<p>成功。</p>

<p>测试多页的美女网页：</p>

<p>先将<code>parse()</code>函数的最后一行改为：<code>yield Request(url='http://www.169bb.com/xiyangmeinv/2016/0717/36463.html', callback=self.demo)</code></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480241935773.png" alt="Alt text" /></p>

<p>都成功。</p>

<hr />

<p>先测试下载图片：</p>

<p>将 <code>pipelines.py</code> 文件 中的 <code>SeconddemoPipeline</code> 类的<code>process_item()</code> 函数里面，添加代码：</p>

<pre><code class="python">    def process_item(self, item, spider):
        # print(len(item['url']))
        for i in range(0, len(item['url'])):
            this_url = item['url'][i]
            id = re.findall('http://724.169pp.net/169mm/(.*?).jpg', this_url)[0]
            id = id.replace('/', '_')
            # print(id)
            file = 'D:/WorkSpace/python_ws/python-large-web-crawler/xiyangmeinv/' + id + '.jpg'
            print('Downloading :' , file)
            urllib.request.urlretrieve(this_url, filename=file)
            print('Final Download :' , file)
        return item
</code></pre>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480242553614.png" alt="Alt text" /></p>

<p>运行程序，没有毛病：（除了下载速度有点慢）</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480242613763.png" alt="Alt text" /></p>

<hr />

<h2>下载 169美女图片网 的所有西洋美女的图片</h2>

<p>在 <code>pic_169bb.py</code>文件里， 将<code>parse()</code>函数的最后一行改为：<code>yield Request(url=xiyang_urldata, callback=self.next)</code></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480242895996.png" alt="Alt text" /></p>

<p>将 <code>demo()</code> 函数里面的所有代码复制一份到 <code>next3()</code>函数里：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480242867282.png" alt="Alt text" /></p>

<p>现在，运行程序：（最终的程序）</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480243169351.png" alt="Alt text" /></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480243201173.png" alt="Alt text" /></p>

<p>成功！</p>

<hr />

<h2>防反爬技术</h2>

<p><strong>Step 4 . </strong> 不遵循 <code>robots.txt</code> 协议。</p>

<p>将 <code>settings.py</code> 文件里面的 <code>ROBOTSTXT_OBEY</code> 项设置为：<code>False</code></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480195250963.png" alt="Alt text" /></p>

<hr />

<p><strong>Step 6 . </strong> 模仿浏览器</p>

<blockquote><p>请先查看这篇博客：<a href="http://blog.csdn.net/github_35160620/article/details/52489709">http://blog.csdn.net/github_35160620/article/details/52489709</a> 里面是：<strong>六 . 设置 用户代理（user_agent）</strong>。</p></blockquote>

<p>将 <code>settings.py</code> 文件里面的 <code>USER_AGENT</code> 项设置为：浏览器的用户代理信息。</p>

<pre><code class="python">USER_AGENT = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.22 Safari/537.36 SE 2.X MetaSr 1.0'
</code></pre>

<hr />

<p><strong>Step 7 . </strong>  禁止缓存</p>

<p>将 <code>settings.py</code> 文件里面的 <code>COOKIES_ENABLED</code> 项设置为：<code>False</code>。</p>

<pre><code class="python">COOKIES_ENABLED = False
</code></pre>

<hr />

<h2>搞定</h2>

<hr />

<p>需要升级的地方:（2016-11-27 19:34:34）</p>

<ol>
<li>在易错的代码段加上异常检测程序</li>
<li>在下载图片的代码加上：超时异常检测程序</li>
<li>记录成功下载的、超时失败下载的、链接失败下载的 信息</li>
<li>添加断点续下功能。</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python3 解决编码问题： UnicodeEncodeError: 'Gbk' Codec Can't Encode Character '\xa9' in Position Xxx: Illegal Multibyte Sequence --- 当执行爬虫将爬取信息打印到终端时出现的编码错误]]></title>
    <link href="http://aobojaing.github.io/blog/2016/12/08/python3-UnicodeEncodeError-gbk-codec-can't-encode-character-xa9/"/>
    <updated>2016-12-08T06:38:45+08:00</updated>
    <id>http://aobojaing.github.io/blog/2016/12/08/python3-UnicodeEncodeError-gbk-codec-can't-encode-character-xa9</id>
    <content type="html"><![CDATA[<hr />

<h2>开发环境</h2>

<ul>
<li>Python第三方库：lxml、Twisted、pywin32、scrapy</li>
<li>Python 版本：python-3.5.0-amd64</li>
<li>PyCharm软件版本：pycharm-professional-2016.1.4</li>
<li>电脑系统：Windows 10 64位</li>
</ul>


<p>如果你还没有搭建好开发环境，请到<a href="http://www.aobosir.com/blog/2016/11/26/python3-large-web-crawler-001-Build-development-environment/">这篇博客</a>。</p>

<hr />

<p>当使用Scrapy写爬虫项目的时候，当我们爬取某些中文网站，然后在DOS终端中打印爬取的网页源代码的时候，会出现各式各样的编码错误，今天，我又遇到一种编码错误，下面我将这个错误和对应的解决办法记录下来。</p>

<p>爬取的目标网址：<a href="http://blog.csdn.net/github_35160620/article/details/53353672">http://blog.csdn.net/github_35160620/article/details/53353672</a></p>

<p>出现错误的代码：</p>

<pre><code class="python">    def next(self, response):
        body_data = response.body.decode('utf-8', 'ignore')
        print(body_data)
        pass
</code></pre>

<p>执行：来到对应的爬虫项目路径下，执行：</p>

<pre><code>scrapy crawl 爬虫名字
</code></pre>

<p>在出现的调试信息中你可以看到一个编码错误：</p>

<pre><code>    print(body_data)
UnicodeEncodeError: 'gbk' codec can't encode character '\xa9' in position 6732: illegal multibyte sequence
</code></pre>

<p>通过查看，这个<a href="http://www.codetable.net/hex/a9"><code>u'xa9'</code> Unicode编码所表示的字符是：<code>©</code></a>。</p>

<p><img src="/images/2016-12-8-python3-UnicodeEncodeError-gbk-codec-can't-encode-character-xa9/1481149755580.png" alt="Alt text" /></p>

<p>可以解决这个错误的方法：</p>

<p>将上面的代码修改为：</p>

<pre><code class="python">    def next(self, response):
        body_data = response.body.decode('utf-8', 'ignore').replace(u'\xa9', u'')
        print(body_data)
        pass
</code></pre>

<p>现在运行这个程序<code>scrapy crawl 爬虫名字 --nolog</code>，上面的编码错误就没有。成功的输出了爬取的网页的源代码。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python3 大型网络爬虫实战 — 给 Scrapy 爬虫项目设置为防反爬]]></title>
    <link href="http://aobojaing.github.io/blog/2016/12/06/python3-large-web-crawler-scrapy-project-Anti-reptile-settings/"/>
    <updated>2016-12-06T00:04:35+08:00</updated>
    <id>http://aobojaing.github.io/blog/2016/12/06/python3-large-web-crawler-scrapy-project-Anti-reptile-settings</id>
    <content type="html"><![CDATA[<hr />

<h2>开发环境</h2>

<ul>
<li>Python第三方库：lxml、Twisted、pywin32、scrapy</li>
<li>Python 版本：python-3.5.0-amd64</li>
<li>PyCharm软件版本：pycharm-professional-2016.1.4</li>
<li>电脑系统：Windows 10 64位</li>
</ul>


<p>如果你还没有搭建好开发环境，请到<a href="http://www.aobosir.com/blog/2016/11/26/python3-large-web-crawler-001-Build-development-environment/">这篇博客</a>。</p>

<hr />

<p>所有的设置都是在scrapy爬虫项目中的<code>settings.py</code> 文件中进行设置。</p>

<p><strong>Step 1 . </strong> 设置爬虫不遵循 <code>robots.txt</code>协议</p>

<pre><code># Obey robots.txt rules
ROBOTSTXT_OBEY = False
</code></pre>

<p><img src="/images/2016-12-6-python3-large-web-crawler-scrapy-project-Anti-reptile-settings/1480952600971.png" alt="Alt text" /></p>

<blockquote><p>想要了解什么是<code>robots.txt</code>协议，请访问这篇博客：<a href="http://blog.csdn.net/github_35160620/article/details/52586126">解析 robots.txt 文件</a>。</p></blockquote>

<p><strong>Step 2 . </strong> 设置取消<strong>Cookies</strong></p>

<pre><code class="python"># Disable cookies (enabled by default)
COOKIES_ENABLED = False
</code></pre>

<p><img src="/images/2016-12-6-python3-large-web-crawler-scrapy-project-Anti-reptile-settings/1480952959564.png" alt="Alt text" /></p>

<blockquote><p><strong>Cookies</strong>：</p>

<p> 简单的说，Cookie就是服务器暂存放在你计算机上的一笔资料，好让服务器用来辨认你的计算机。当你在浏览网站的时候，Web服务器会先送一小小资料放在你的计算机上，Cookie 会帮你在网站上所打的文字或是一些选择，都记录下来。当下次你再光临同一个网站，Web服务器会先看看有没有它上次留下的Cookie资料，有的话，就会依据Cookie里的内容来判断使用者，送出特定的网页内容给你。</p></blockquote>

<p><strong>Step 3 . </strong> 设置用户代理值（<code>USER_AGENT</code>）</p>

<pre><code class="python"># Crawl responsibly by identifying yourself (and your website) on the user-agent
USER_AGENT = 'Mozilla/xxx (Windows xxx; Winxx; xxx) AppleWebKit/xxx (KHTML, like Gecko) Chrome/xxxx Safari/xxx'
</code></pre>

<p><img src="/images/2016-12-6-python3-large-web-crawler-scrapy-project-Anti-reptile-settings/1480953048379.png" alt="Alt text" /></p>

<p>这个 用户代理可以在浏览器里面找到：</p>

<p>随便浏览一个网页，按<strong>F12</strong> -> <strong>Network</strong> -> <strong>F5</strong>，随便点击一项，你都能看到有 <strong>User-agent</strong> 这一项，将这里面的内容拷贝就可以。</p>

<p><img src="/images/2016-12-6-python3-large-web-crawler-scrapy-project-Anti-reptile-settings/1480953359818.png" alt="Alt text" /></p>

<p><strong>Step 4 . </strong> 设置IP</p>

<p>对于这一步，如果你没有做什么违法的事情，可以不用设置。仅仅上面的三个步骤，就可以将那些具有反爬虫机制的网站可以正常爬取了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python3 大型网络爬虫实战 002 --- Scrapy 爬虫项目的创建及爬虫的创建 --- 实例：爬取百度标题和CSDN博客]]></title>
    <link href="http://aobojaing.github.io/blog/2016/11/26/python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn/"/>
    <updated>2016-11-26T18:22:27+08:00</updated>
    <id>http://aobojaing.github.io/blog/2016/11/26/python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn</id>
    <content type="html"><![CDATA[<hr />

<h2>开发环境</h2>

<ul>
<li>Python第三方库：lxml、Twisted、pywin32、scrapy</li>
<li>Python 版本：python-3.5.0-amd64</li>
<li>PyCharm软件版本：pycharm-professional-2016.1.4</li>
<li>电脑系统：Windows 10 64位</li>
</ul>


<p>如果你还没有搭建好开发环境，请到<a href="http://www.aobosir.com/blog/2016/11/26/python3-large-web-crawler-001-Build-development-environment/">这篇博客</a>。</p>

<hr />

<h1>1 知识点：scrapy 爬虫项目的创建及爬虫的创建</h1>

<h2>1.1 scrapy 爬虫项目的创建</h2>

<p>接下来我们为大家创建一个Scrapy爬虫项目，并在爬虫项目下创建一个Scrapy爬虫文件。</p>

<pre><code>scrapy startproject &lt;projectname&gt; 
</code></pre>

<h2>1.2 scrapy 爬虫文件的创建</h2>

<pre><code>cd demo
scrapy genspider -t basic &lt;filename&gt; &lt;domain&gt;
</code></pre>

<blockquote><p>更多 <strong>Scrapy</strong> 命令的介绍请到<a href="http://www.aobosir.com/blog/2016/11/26/python-Scrapy-command/">这篇博客</a>查看。</p></blockquote>

<hr />

<h1>2 实例：爬取百度标题和CSDN博客</h1>

<p>我们创建一个爬虫项目，在里面创建一个爬虫文件来爬取百度，并再创建一个爬虫文件爬取CSDN博客文章。</p>

<hr />

<p>先创建一个Scrapy爬虫项目：</p>

<pre><code>scrapy startproject firstDemo
</code></pre>

<p>输出：</p>

<pre><code>D:\WorkSpace\python_ws\python-large-web-crawler&gt;scrapy startproject firstdemo
New Scrapy project 'firstdemo', using template directory 'c:\\users\\aobo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\scrapy\\templates\\project', created in:
    D:\WorkSpace\python_ws\python-large-web-crawler\firstdemo

You can start your first spider with:
    cd firstdemo
    scrapy genspider example example.com

D:\WorkSpace\python_ws\python-large-web-crawler&gt;
</code></pre>

<h2>2-1.1 使用Scrapy爬虫 爬取百度标题</h2>

<p>创建一个爬虫文件来爬取百度</p>

<pre><code>cd firstDemo
scrapy genspider -t basic baidu baidu.com
</code></pre>

<p>输出：</p>

<pre><code>D:\WorkSpace\python_ws\python-large-web-crawler&gt;cd firstdemo

D:\WorkSpace\python_ws\python-large-web-crawler\firstdemo&gt;scrapy genspider -t basic baidu baidu.com
Created spider 'baidu' using template 'basic' in module:
  firstdemo.spiders.baidu

D:\WorkSpace\python_ws\python-large-web-crawler\firstdemo&gt;
</code></pre>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn/1480084611306.png" alt="Alt text" /></p>

<p>打开 <strong>PyCharm</strong> 软件，用 <strong>PyCharm</strong> 软件打开刚刚创建的 <code>firstdemo</code> 爬虫项目。</p>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn//1480084757859.png" alt="Alt text" /></p>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn//1480084783287.png" alt="Alt text" /></p>

<p>打开这 <code>baidu.py</code> 爬虫文件，你会看到自动生成的代码：</p>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn//1480084952255.png" alt="Alt text" /></p>

<h2>2-1.2 观察 <a href="https://www.baidu.com/">https://www.baidu.com/</a> 网页源代码</h2>

<p>（源代码太多，列出重点的。）</p>

<pre><code class="http">&lt;html xmlns="http://www.w3.org/1999/xhtml" class="cye-enabled cye-nm sui-componentWrap"&gt;
    &lt;head&gt;
        &lt;title&gt;百度一下，你就知道 &lt;/title&gt;
    &lt;/head&gt;
&lt;/html&gt;
</code></pre>

<p>源代码中的标题通过标签逐步定位： <code>/html/head/title</code></p>

<h2>2-1.3 写代码</h2>

<p>我们现在要提取出 <a href="https://www.baidu.com/">https://www.baidu.com/</a> 网页 的标题：<strong>百度一下，你就知道</strong>。</p>

<p>提取信息，一般使用 <code>xpath</code> 或者 <strong>正则表达式</strong> 来提取。</p>

<p>这里我们使用 <code>xpath</code> 来提取，<code>xpath</code> 的知识点，请到<a href="http://www.aobosir.com/blog/2016/11/26/python-xpath/">这篇博客</a>中查看。</p>

<hr />

<p>下面的编写代码的步骤：</p>

<p><strong>Step 1 . </strong> 设置我们的爬虫不遵循 <code>robots.txt</code> 规定。（什么是<code>robots.txt</code>规定，请到<a href="http://blog.csdn.net/github_35160620/article/details/52586126">这个博客</a>查看。）</p>

<p>打开 <code>settings.py</code> 文件，将里面的<code>ROBOTSTXT_OBEY</code> 设为：<code>False</code></p>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn//1480086841701.png" alt="Alt text" /></p>

<p><strong>Step 2 . </strong>  打开 <code>items.py</code> 文件，在里面<code>FirstdemoItem()</code>函数里添加一项：</p>

<pre><code>    title = scrapy.Field()
</code></pre>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn//1480087390745.png" alt="Alt text" /></p>

<p><strong>Step 3 . </strong>  在 <code>baidu.py</code> 文件里面，使用<code>xpath 表达式</code> 提取百度网页的标题。</p>

<p>先从核心目录（<code>firstdemo</code>）定位到<code>items.py</code> 文件里面的<code>FirstdemoItem</code>函数。</p>

<p>然后使用<code>xpath 表达式</code> 提取百度网页的标题。</p>

<p>最后，返回。</p>

<pre><code class="python"># -*- coding: utf-8 -*-
import scrapy
from firstdemo.items import FirstdemoItem

class BaiduSpider(scrapy.Spider):
    name = "baidu"
    allowed_domains = ["baidu.com"]
    start_urls = ['http://baidu.com/']

    def parse(self, response):
        item = FirstdemoItem()
        item['title'] = response.xpath('/html/head/title/text()').extract()
        yield item
</code></pre>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn//1480088827529.png" alt="Alt text" /></p>

<p><strong>Step 4 . </strong>
在 <code>pipelines.py</code> 文件里面的<code>FirstdemoPipeline()</code>函数，添加打印信息的代码：</p>

<pre><code class="python">        print(item['title'])
</code></pre>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn//1480087767661.png" alt="Alt text" /></p>

<p>但是，现在运行程序，是不能输出任何信息的，还需要做<strong>Step 5</strong>。</p>

<p><strong>Step 5 . </strong> 开启<code>piplines</code>（默认<code>piplines</code>是关闭的。）
在 <code>settings.py</code> 文件，将里面的<code>ITEM_PIPELINES</code> 项的注释去掉。并从核心目录开始定位，定位到<code>pipelines.py</code> 文件里面的<code>FirstdemoPipeline()</code>函数 ，就应该是：<code>firstdemo.pipelines.FirstdemoPipeline</code>：</p>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn//1480088423683.png" alt="Alt text" /></p>

<h2>2-1.4 运行</h2>

<p>在 <strong>DOS窗口</strong> 中，先将路劲切换到当前爬虫项目<code>firstdemo</code>路径下，然后在执行爬虫文件 <code>baidy</code></p>

<pre><code>D:
cd D:\WorkSpace\python_ws\python-large-web-crawler\firstdemo
scrapy crawl baidu --nolog
</code></pre>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn//1480089009819.png" alt="Alt text" /></p>

<hr />

<h2>2-2.1 使用Scrapy爬虫CSDN的博客文章</h2>

<p>创建一个爬虫文件爬取CSDN博客文章。</p>

<pre><code>scrapy genspider -t basic csdn blog.csdn.net
</code></pre>

<p>输出:</p>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn//1480089722224.png" alt="Alt text" /></p>

<h2>2-2.2 观察 <a href="http://blog.csdn.net/">http://blog.csdn.net/</a> 网页源代码</h2>

<p>（网页源代码太多，这里就不贴出了。）</p>

<p>提取信息，一般使用 <code>xpath</code> 或者 <strong>正则表达式</strong> 来提取。</p>

<h2>2-2.3 写代码</h2>

<p><strong>Step 1 . </strong>  在<code>items.py</code> 文件中的<code>FirstdemoItem()</code>函数中添加新的项。其他的文件会使用这几个对象：</p>

<pre><code class="python">    detail = scrapy.Field()
    link = scrapy.Field()
</code></pre>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn//1480089896612.png" alt="Alt text" /></p>

<p><strong>Step 2 . </strong>  在 <code>csdn.py</code> 文件里面，使用<code>xpath 表达式</code> 提取csdn博客网页的博文标题、介绍、链接地址。</p>

<pre><code class="python"># -*- coding: utf-8 -*-
import scrapy
from firstdemo.items import FirstdemoItem


class CsdnSpider(scrapy.Spider):
    name = "csdn"
    allowed_domains = ["blog.csdn.net"]
    start_urls = ['http://blog.csdn.net/']

    def parse(self, response):
        item = FirstdemoItem()
        item['title'] = response.xpath("//h3[@class='tracking-ad']/a/text()").extract()
        item['detail'] = response.xpath("//div[@class='blog_list_c']/text()").extract()
        item['link'] = response.xpath("//h3[@class='tracking-ad']/a/@href").extract()
        yield item
</code></pre>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn//1480090250536.png" alt="Alt text" /></p>

<p><strong>Step 3 . </strong> 在 <code>piplines.py</code> 文件中，添加下面的代码，输出显示爬取到的信息。</p>

<pre><code class="python">        for i in range(0, len(item['title'])):
            print('第' + str(i+1) + '篇文章：')
            print(item['title'][i])
            print(item['detail'][i])
            print(item['link'][i])
            print('---------')
</code></pre>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn//1480090494545.png" alt="Alt text" /></p>

<h2>2-1.4 运行</h2>

<pre><code>scrapy crawl csdn --nolog
</code></pre>

<blockquote><p>执行输出的信息太少，说明程序有问题。</p>

<pre><code>scrapy crawl csdn
</code></pre>

<p>如果你在执行的时候，找到错误提示信息：</p>

<pre><code>UnicodeEncodeError: 'gbk' codec can't encode character '\xa0' in position 10: illegal multibyte sequence
</code></pre>

<p>这个问题经常会遇到，是一个常见的问题，解决办法<a href="http://www.aobosir.com/blog/2016/11/26/python3-UnicodeEncodeError-gbk-codec-can%27t-encode-character-xa0/">在这里</a>可以找到。</p>

<pre><code class="python">         print(item['detail'][i].replace(u'\xa0 ', u' '))
</code></pre></blockquote>

<p>输出：</p>

<pre><code>第1篇文章：
微信小程序：小程序，新场景
前言：我们频繁进入的地方，是场景。手机，是场景；浏览器，是场景；其实，微信，也是场景……微信要做的是占据更多用户时间、占
据更多应用场景、占据更多服务入口，这是商业本质想去垄断要做的事情。对于大家来讲，...
http://blog.csdn.net/liujia216/article/details/53350247
---------
第2篇文章：
Android四大组件——BroadcastReceiver普通广播、有序广播、拦截广播、本地广播、Sticky广播、系统广播
BroadcastReceiver普通广播、有序广播、拦截广播、本地广播、Sticky广播、系统广播
  本篇文章包括以下内容：


  前言
  BroadcastReceiver的简介
 ...
http://blog.csdn.net/qq_30379689/article/details/53341313
---------
第3篇文章：
Gif格式简要介绍
Gif格式的介绍

为什么有的Gif图不能够循环播放及处理办法
http://blog.csdn.net/shiroh_ms08/article/details/53347873
---------
第4篇文章：
win10 uwp 打包第三方字体到应用
有时候我们会把一些特殊字体打包到软件，因为如果找不到我们的字体会变为默认，现在很多字体图标我们用得好，有时候我们的应用会
用很漂亮的字体，需要我们自己打包，因为用户一般是没有字体。UWP使用第三方字体首...
http://blog.csdn.net/lindexi_gd/article/details/52716655
---------
第5篇文章：
话说智能指针发展之路
动态创建内存的管理太麻烦，于是乎，这个世界变成11派人：
一派人勤勤恳恳按照教科书的说法做，时刻小心翼翼，苦逼连连；
一派人忘记教科书的教导，随便乱来，搞得代码处处bug，后期维护骂声连连；
最...
http://blog.csdn.net/jacketinsysu/article/details/53343534
---------
第6篇文章：
安卓自定义控件（二）BitmapShader、ShapeDrawable、Shape
第一篇博客中，我已经对常用的一些方法做了汇总，这篇文章主要介绍BitmapShader位图渲染、ComposeShader组合渲染，然后看看Xferm
ode如何实际应用。不过本文还是只重写onDraw...
http://blog.csdn.net/chen413203144/article/details/53343209
---------
第7篇文章：
JSTL 标签大全详解
1、什么是JSTL？    JSTL是apache对EL表达式的扩展（也就是说JSTL依赖EL），JSTL是标签语言！JSTL标签使用以来非常方便，它与JSP
动作标签一样，只不过它不是JSP内...
http://blog.csdn.net/qq_25827845/article/details/53311722
---------
第8篇文章：
Android调试大法 自定义IDE默认签名文件
你是否为调试第三方SDK时debug签名和release签名发生冲突而烦恼？你是否在debug时第三方功能测试通过，而release时无法使用？你
是否在为对接微信、支付宝、地图因签名导致的问题而烦恼？...
http://blog.csdn.net/yanzhenjie1003/article/details/53334071
---------
第9篇文章：
Android图表库MPAndroidChart(十二)——来点不一样的，正负堆叠条形图
Android图表库MPAndroidChart(十二)——来点不一样的，正负堆叠条形图
  接上篇，今天要说的，和上篇的类似，只是方向是有相反的两面，我们先看下效果  实际上这样就导致了我们的代码是...
http://blog.csdn.net/qq_26787115/article/details/53333270
---------
第10篇文章：
一步步手动实现热修复(二)-类的加载机制简要介绍
一个类在被加载到内存之前要经过加载、验证、准备等过程。经过这些过程之后，虚拟机才会从方法区将代表类的运行时数据结构转换为
内存中的Class。

我们这节内容的重点在于一个类是如何被加载的，所以我们从类...
http://blog.csdn.net/sahadev_/article/details/53334911
---------
第11篇文章：
仿射变换详解 warpAffine
今天遇到一个问题是关于仿射变换的，但是由于没有将仿射变换的具体原理型明白，看别人的代码看的很费解，最后终于在师兄的帮助下
将原理弄明白了，我觉得最重要的是理解仿射变换可以看成是几种简单变换的复合实现，
...
http://blog.csdn.net/q123456789098/article/details/53330484
---------
第12篇文章：
React Native嵌入Android原生应用中
开发环境准备首先你要搭建好React Native for Android开发环境， 没有搭建好的可以参考：React Native for Android Windows环境
搭建  用Android...
http://blog.csdn.net/u011965040/article/details/53331859
---------
第13篇文章：
TCP三次握手四次挥手详解
TCP三次握手四次挥手详解
http://blog.csdn.net/u010913001/article/details/53331863
---------
第14篇文章：
腾讯Android面经
秋招收官最后一战。
腾讯一面（电话）：
自我介绍
项目，平时怎么学习？
设计模式
（1）知道哪些设计模式？设计模式在Android、Java中是怎么应用的，每个都说一下？
（2）InputStre...
http://blog.csdn.net/kesarchen/article/details/53332157
---------
第15篇文章：
轻松实现部分背景半透明的呈现效果
实现一个简单的呈现/解散动画效果，当呈现时，呈现的主要内容和背景要明显区分，背景呈现一个半透明遮罩效果，透过背景可以看到
下层 View Controller 的内容
http://blog.csdn.net/kmyhy/article/details/53322669
---------
第16篇文章：
APP自动化框架LazyAndroid使用手册（4）--测试模板工程详解
概述前面的3篇博文分别对lazyAndroid的框架简介、元素抓取和核心API进行了说明，本文将基于框架给出的测试模板工程，详细阐述下
使用该框架进行安卓UI自动化测试的步骤。
http://blog.csdn.net/kaka1121/article/details/53325265
---------
第17篇文章：
Android使用getIdentifier()方法根据资源名来获取资源id
有时候我们想动态的根据一个资源名获得到对应的资源id，就可以使用getResources().getIdentifier()方法来获取该id。然后再使用该
id进行相关的操作。
1、Demo示例
  下...
http://blog.csdn.net/ouyang_peng/article/details/53328000
---------
第18篇文章：
Android基于RecyclerView实现高亮搜索列表
这篇应该是RecycleView的第四篇了，RecycleView真是新生代的宠儿能做这么多的事情。转载请注明作者AndroidMsky及原文链接
http://blog.csdn.net/and...
http://blog.csdn.net/androidmsky/article/details/53306657
---------
第19篇文章：
使用Git Hooks实现开发部署任务自动化
提供：ZStack云计算 前言版本控制，这是现代软件开发的核心需求之一。有了它，软件项目可以安全的跟踪代码变更并执行回溯、完整
性检查、协同开发等多种操作。在各种版本控制软件中，git是近年来最流行的软...
http://blog.csdn.net/zstack_org/article/details/53331077
---------
第20篇文章：
Andromeda OS 来了，Android 再见？
相信有部分同学已经有耳闻了，前几天炒的很火一个消息，就是 Google 要推出一种全新的操作系统，取名 Andromeda，这款新型的操作
系统融合了 Android 和 Chrome OS，据称已经有...
http://blog.csdn.net/googdev/article/details/53331364
---------
</code></pre>

<hr />

<p>我用英语跟小贩交谈，突然画面一下就全暗，我回台上，终于轮我上场。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python --- Xpath 表达式 --- Ongoing]]></title>
    <link href="http://aobojaing.github.io/blog/2016/11/26/python-xpath/"/>
    <updated>2016-11-26T18:00:15+08:00</updated>
    <id>http://aobojaing.github.io/blog/2016/11/26/python-xpath</id>
    <content type="html"><![CDATA[<hr />

<h2>什么是 <strong>XPath</strong>？</h2>

<p>参考网站：<a href="http://www.w3school.com.cn/xpath/xpath_intro.asp">http://www.w3school.com.cn/xpath/xpath_intro.asp</a></p>

<p>XPath 是一门在 XML 文档中查找信息的语言。XPath 用于在 XML 文档中通过元素和属性进行导航。</p>

<h2>xpath 表达式 语法讲解</h2>

<p>参考网站：<a href="http://www.w3school.com.cn/xpath/xpath_syntax.asp">http://www.w3school.com.cn/xpath/xpath_syntax.asp</a></p>

<p>例如，现在有这些信息：</p>

<pre><code>&lt;head&gt;
&lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt;
&lt;title&gt;CSDN.NET - 全球最大中文IT社区，为IT专业技术人员提供最全面的信息传播和服务平台&lt;/title&gt;
&lt;link href="http://c.csdnimg.cn/www/css/csdn_common.css" rel="stylesheet" type="text/css"&gt;
&lt;link href="css/content.css" rel="stylesheet" type="text/css"&gt;
&lt;link href="http://c.csdnimg.cn/public/favicon.ico" rel="SHORTCUT ICON"&gt;
&lt;/head&gt;
</code></pre>

<table>
<thead>
<tr>
<th style="text-align:left;"> 表达式</th>
<th style="text-align:right;">     意思</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">/ </td>
<td style="text-align:right;">寻找指定标签</td>
</tr>
<tr>
<td style="text-align:left;">//</td>
<td style="text-align:right;"> 寻找所有标签</td>
</tr>
<tr>
<td style="text-align:left;">@</td>
<td style="text-align:right;"> 提取某个标签的属性的内容 </td>
</tr>
</tbody>
</table>


<hr />

<p><strong>例子：</strong></p>

<p>不管怎么样，记住一点：<strong>只有唯一的东西才能定位</strong>。</p>

<pre><code class="python">/title.text()
</code></pre>

<p>表示：</p>

<pre><code>CSDN.NET - 全球最大中文IT社区，为IT专业技术人员提供最全面的信息传播和服务平台
</code></pre>

<hr />

<p>参考网站：<a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/selectors.html">http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/selectors.html</a></p>

<hr />
]]></content>
  </entry>
  
</feed>
