<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: python | AoboSir 博客]]></title>
  <link href="http://aobojaing.github.io/tags/python/atom.xml" rel="self"/>
  <link href="http://aobojaing.github.io/"/>
  <updated>2016-11-29T19:21:14+08:00</updated>
  <id>http://aobojaing.github.io/</id>
  <author>
    <name><![CDATA[Aobo Jaing]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Learning Python 015 解决问题：读取文件时，出现乱码或者“UnicodeDecodeError  'gbk' codec can't decode byte 0xXX in position XX: incomplete multibyte sequence” 错误]]></title>
    <link href="http://aobojaing.github.io/blog/2016/11/29/Python-read-file-Garbled-UnicodeDecodeError-gbk-codec-cant-decode-error/"/>
    <updated>2016-11-29T19:18:27+08:00</updated>
    <id>http://aobojaing.github.io/blog/2016/11/29/Python-read-file-Garbled-UnicodeDecodeError-gbk-codec-cant-decode-error</id>
    <content type="html"><![CDATA[<hr />

<ul>
<li>使用的电脑系统：Windows 10 64位</li>
<li>使用的开发集成环境：PyCharm 2016.1.4</li>
<li>使用的Python的版本：python 3.5.0</li>
</ul>


<hr />

<h2>出现的错误</h2>

<p>读取文件时，出现<strong>乱码</strong>或者<code>UnicodeDecodeError: 'gbk' codec can't decode byte 0xXX in position XX: incomplete multibyte sequence</code> 错误</p>

<hr />

<h2>出现错误的原因</h2>

<p>这两个错误可能会出现一个，两个错误的出现的原因是一样的：当我们使用了一个不正确的编码方式去读取一个不是用这个编码方式编码的文件时，轻者出现乱码，重者出现<code>UnicodeDecodeError</code>错误。</p>

<hr />

<h2>模拟错误发生现场</h2>

<pre><code class="python">file = open('newfile.txt', 'w', encoding='utf-8')
file.write('你好，AoboSir.')
file.close()
file = open('newfile.txt', 'r')
print(file.read())
file.close()
</code></pre>

<p>运行输出：</p>

<pre><code>浣犲ソ锛孉oboSir.
</code></pre>

<hr />

<pre><code class="python">file = open('newfile.txt', 'w', encoding='utf-8')
file.write('你好，AoboSir。')
file.close()
file = open('newfile.txt', 'r')
print(file.read())
file.close()
</code></pre>

<p>运行输出：</p>

<pre><code>Traceback (most recent call last):
  File "D:/WorkSpace/test_ws/demo/learning_python_15.py", line 6, in &lt;module&gt;
    print(file.read())
UnicodeDecodeError: 'gbk' codec can't decode byte 0x82 in position 35: incomplete multibyte sequence
</code></pre>

<hr />

<h2>解决办法</h2>

<p>读取文件时，指定正确的编码方式：</p>

<pre><code class="python">file = open('newfile.txt', 'r', encoding='utf-8')
</code></pre>

<p>现在再运行，就正常了：</p>

<pre><code>你好，AoboSir。
</code></pre>

<hr />

<h2>搞定</h2>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python --- xpath 表达式 --- Ongoing]]></title>
    <link href="http://aobojaing.github.io/blog/2016/11/26/python-xpath/"/>
    <updated>2016-11-26T18:00:15+08:00</updated>
    <id>http://aobojaing.github.io/blog/2016/11/26/python-xpath</id>
    <content type="html"><![CDATA[<hr />

<h2>什么是 <strong>XPath</strong>？</h2>

<p>参考网站：<a href="http://www.w3school.com.cn/xpath/xpath_intro.asp">http://www.w3school.com.cn/xpath/xpath_intro.asp</a></p>

<p>XPath 是一门在 XML 文档中查找信息的语言。XPath 用于在 XML 文档中通过元素和属性进行导航。</p>

<h2>xpath 表达式 语法讲解</h2>

<p>参考网站：<a href="http://www.w3school.com.cn/xpath/xpath_syntax.asp">http://www.w3school.com.cn/xpath/xpath_syntax.asp</a></p>

<p>例如，现在有这些信息：</p>

<pre><code>&lt;head&gt;
&lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt;
&lt;title&gt;CSDN.NET - 全球最大中文IT社区，为IT专业技术人员提供最全面的信息传播和服务平台&lt;/title&gt;
&lt;link href="http://c.csdnimg.cn/www/css/csdn_common.css" rel="stylesheet" type="text/css"&gt;
&lt;link href="css/content.css" rel="stylesheet" type="text/css"&gt;
&lt;link href="http://c.csdnimg.cn/public/favicon.ico" rel="SHORTCUT ICON"&gt;
&lt;/head&gt;
</code></pre>

<table>
<thead>
<tr>
<th style="text-align:left;"> 表达式</th>
<th style="text-align:right;">     意思</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">/ </td>
<td style="text-align:right;">寻找指定标签</td>
</tr>
<tr>
<td style="text-align:left;">//</td>
<td style="text-align:right;"> 寻找所有标签</td>
</tr>
<tr>
<td style="text-align:left;">@</td>
<td style="text-align:right;"> 提取某个标签的属性的内容 </td>
</tr>
</tbody>
</table>


<hr />

<p><strong>例子：</strong></p>

<p>不管怎么样，记住一点：<strong>只有唯一的东西才能定位</strong>。</p>

<pre><code class="python">/title.text()
</code></pre>

<p>表示：</p>

<pre><code>CSDN.NET - 全球最大中文IT社区，为IT专业技术人员提供最全面的信息传播和服务平台
</code></pre>

<hr />

<p>参考网站：<a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/selectors.html">http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/selectors.html</a></p>

<hr />
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[解决问题： pywin32 安装后出现 import win32api ImportError  DLL load failed]]></title>
    <link href="http://aobojaing.github.io/blog/2016/11/26/solve-pywin32-import-win32api-ImportError-DLL-load-failed/"/>
    <updated>2016-11-26T07:04:25+08:00</updated>
    <id>http://aobojaing.github.io/blog/2016/11/26/solve-pywin32-import-win32api-ImportError-DLL-load-failed</id>
    <content type="html"><![CDATA[<hr />

<p>执行 <code>scrapy bench</code> 命令时 出现错误。（之前安装了pywin32库）</p>

<pre><code>Traceback (most recent call last):
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\twisted\internet\defer.py", line 1260, in _inlineCallbacks
    result = g.send(result)
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\scrapy\crawler.py", line 72, in crawl
    self.engine = self._create_engine()
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\scrapy\crawler.py", line 97, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\scrapy\core\engine.py", line 68, in __init__
    self.downloader = downloader_cls(crawler)
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "&lt;frozen importlib._bootstrap&gt;", line 986, in _gcd_import
  File "&lt;frozen importlib._bootstrap&gt;", line 969, in _find_and_load
  File "&lt;frozen importlib._bootstrap&gt;", line 958, in _find_and_load_unlocked
  File "&lt;frozen importlib._bootstrap&gt;", line 673, in _load_unlocked
  File "&lt;frozen importlib._bootstrap_external&gt;", line 662, in exec_module
  File "&lt;frozen importlib._bootstrap&gt;", line 222, in _call_with_frames_removed
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\scrapy\downloadermiddlewares\retry.py", line 23, in &lt;module&gt;
    from scrapy.xlib.tx import ResponseFailed
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\scrapy\xlib\tx\__init__.py", line 3, in &lt;module&gt;
    from twisted.web import client
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\twisted\web\client.py", line 42, in &lt;module&gt;
    from twisted.internet.endpoints import TCP4ClientEndpoint, SSL4ClientEndpoint
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\twisted\internet\endpoints.py", line 36, in &lt;module&gt;
    from twisted.internet.stdio import StandardIO, PipeAddress
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\twisted\internet\stdio.py", line 30, in &lt;module&gt;
    from twisted.internet import _win32stdio
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\twisted\internet\_win32stdio.py", line 9, in &lt;module&gt;
    import win32api
ImportError: DLL load failed: 找不到指定的模块。
</code></pre>

<h2>解决办法：</h2>

<p>参考网站：</p>

<p><a href="http://blog.csdn.net/mtt_sky/article/details/50445938">http://blog.csdn.net/mtt_sky/article/details/50445938</a>
<a href="http://blog.sina.com.cn/s/blog_5a81b7990101l225.html">http://blog.sina.com.cn/s/blog_5a81b7990101l225.html</a></p>

<p><code>C:\Users\AOBO\AppData\Local\Programs\Python\Python35\Lib\site-packages\pywin32_system32</code></p>

<p>里面的所有的文件复制到：<code>C:\Windows\System32</code></p>

<p>现在，问题解决。无需重新打开DOS窗口，直接执行：<code>scrapy bench</code>。</p>

<p>输出正常：</p>

<pre><code>D:\BaiduYunDownload\first&gt;scrapy bench
2016-11-23 13:56:45 [scrapy] INFO: Scrapy 1.2.1 started (bot: first)
2016-11-23 13:56:45 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'first.spiders', 'CLOSESPIDER_TIMEOUT': 10, 'LOGSTATS_INTERVAL': 1, 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'first', 'SPIDER_MODULES': ['first.spiders']}
2016-11-23 13:56:47 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.closespider.CloseSpider',
 'scrapy.extensions.logstats.LogStats']
2016-11-23 13:56:48 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-23 13:56:48 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-23 13:56:48 [scrapy] INFO: Enabled item pipelines:
['first.pipelines.FirstPipeline']
2016-11-23 13:56:48 [scrapy] INFO: Spider opened
2016-11-23 13:56:48 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-23 13:56:49 [scrapy] INFO: Crawled 69 pages (at 4140 pages/min), scraped 0 items (at 0 items/min)
2016-11-23 13:56:50 [scrapy] INFO: Crawled 141 pages (at 4320 pages/min), scraped 0 items (at 0 items/min)
2016-11-23 13:56:51 [scrapy] INFO: Crawled 205 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
2016-11-23 13:56:52 [scrapy] INFO: Crawled 269 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
2016-11-23 13:56:53 [scrapy] INFO: Crawled 325 pages (at 3360 pages/min), scraped 0 items (at 0 items/min)
2016-11-23 13:56:54 [scrapy] INFO: Crawled 373 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)
2016-11-23 13:56:55 [scrapy] INFO: Crawled 429 pages (at 3360 pages/min), scraped 0 items (at 0 items/min)
2016-11-23 13:56:56 [scrapy] INFO: Crawled 477 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)
2016-11-23 13:56:57 [scrapy] INFO: Crawled 533 pages (at 3360 pages/min), scraped 0 items (at 0 items/min)
2016-11-23 13:56:58 [scrapy] INFO: Crawled 581 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)
2016-11-23 13:56:58 [scrapy] INFO: Closing spider (closespider_timeout)
2016-11-23 13:56:59 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 265444,
 'downloader/request_count': 597,
 'downloader/request_method_count/GET': 597,
 'downloader/response_bytes': 1833261,
 'downloader/response_count': 597,
 'downloader/response_status_count/200': 597,
 'finish_reason': 'closespider_timeout',
 'finish_time': datetime.datetime(2016, 11, 23, 5, 56, 59, 266168),
 'log_count/INFO': 17,
 'request_depth_max': 20,
 'response_received_count': 597,
 'scheduler/dequeued': 597,
 'scheduler/dequeued/memory': 597,
 'scheduler/enqueued': 11938,
 'scheduler/enqueued/memory': 11938,
 'start_time': datetime.datetime(2016, 11, 23, 5, 56, 48, 450531)}
2016-11-23 13:56:59 [scrapy] INFO: Spider closed (closespider_timeout)

D:\BaiduYunDownload\first&gt;
</code></pre>

<hr />
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python --- Scrapy 命令]]></title>
    <link href="http://aobojaing.github.io/blog/2016/11/26/python-Scrapy-command/"/>
    <updated>2016-11-26T06:39:53+08:00</updated>
    <id>http://aobojaing.github.io/blog/2016/11/26/python-Scrapy-command</id>
    <content type="html"><![CDATA[<hr />

<p>Scrapy 命令 分为两种：<strong>全局命令</strong> 和 <strong>项目命令</strong>。</p>

<p>全局命令：在哪里都能使用。</p>

<p>项目命令：必须在爬虫项目里面才能使用。</p>

<h2>全局命令</h2>

<pre><code>C:\Users\AOBO&gt;scrapy -h
Scrapy 1.2.1 - no active project

Usage:
  scrapy &lt;command&gt; [options] [args]

Available commands:
  bench         Run quick benchmark test
  commands
  fetch         Fetch a URL using the Scrapy downloader
  genspider     Generate new spider using pre-defined templates
  runspider     Run a self-contained spider (without creating a project)
  settings      Get settings values
  shell         Interactive scraping console
  startproject  Create new project
  version       Print Scrapy version
  view          Open URL in browser, as seen by Scrapy

  [ more ]      More commands available when run from project directory

Use "scrapy &lt;command&gt; -h" to see more info about a command
</code></pre>

<ul>
<li><strong>startproject</strong>：创建一个爬虫项目：<code>scrapy startproject demo</code>（<code>demo</code> 创建的爬虫项目的名字）</li>
<li><strong>runspider</strong> 运用单独一个爬虫文件：<code>scrapy runspider abc.py</code></li>
<li><strong>veiw</strong> 下载一个网页的源代码，并在默认的文本编辑器中打开这个源代码：<code>scrapy view http://www.aobossir.com/</code></li>
<li><strong>shell</strong> 进入交互终端，用于爬虫的调试（如果你不调试，那么就不常用）：<code>scrapy shell http://www.baidu.com --nolog</code>（<code>--nolog</code> 不显示日志信息）</li>
<li><strong>version</strong> 查看版本：（<code>scrapy version</code>）</li>
<li><strong>bench</strong> 测试本地硬件性能（工作原理：）：<code>scrapy bench</code> （如果遇到问题：解决问题:  <code>import win32api ImportError: DLL load failed</code>，到<a href="http://www.aobosir.com/blog/2016/11/26/solve-pywin32-import-win32api-ImportError-DLL-load-failed/">这里</a>查看解决办法。）</li>
</ul>


<hr />

<h2>项目命令</h2>

<p>（进入项目路径，才能看到项目命令）</p>

<pre><code>D:\BaiduYunDownload\first&gt;scrapy -h
Scrapy 1.2.1 - project: first

Usage:
  scrapy &lt;command&gt; [options] [args]

Available commands:
  bench         Run quick benchmark test
  check         Check spider contracts
  commands
  crawl         Run a spider
  edit          Edit spider
  fetch         Fetch a URL using the Scrapy downloader
  genspider     Generate new spider using pre-defined templates
  list          List available spiders
  parse         Parse URL (using its spider) and print the results
  runspider     Run a self-contained spider (without creating a project)
  settings      Get settings values
  shell         Interactive scraping console
  startproject  Create new project
  version       Print Scrapy version
  view          Open URL in browser, as seen by Scrapy

Use "scrapy &lt;command&gt; -h" to see more info about a command

D:\BaiduYunDownload\first&gt;
</code></pre>

<ul>
<li><strong>genspider</strong> 创建一个爬虫文件，我们在爬虫项目里面才能创建爬虫文件（这个命令用的非常多）（<strong>startproject</strong>：创建一个爬虫项目）。创建爬虫文件是按照以下模板来创建的，使用<code>scrapy genspider -l</code> 命令查看有哪些模板。</li>
</ul>


<pre><code>D:\BaiduYunDownload\first&gt;scrapy genspider -l
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed

D:\BaiduYunDownload\first&gt;
</code></pre>

<blockquote><p><code>basic</code> 基础
<code>crawl</code>自动爬虫
<code>csvfeed</code>用来处理csv文件
<code>xmlfeed</code>用来处理xml文件</p>

<p>按照<code>basic</code>模板创建一个名为<code>f1</code>的爬虫文件：<code>scrapy genspider -t basic f1</code> ，创建了一个<code>f1.py</code>文件。</p></blockquote>

<ul>
<li><p><strong>check</strong> 测试爬虫文件、或者说：检测一个爬虫，如果结果是：OK，那么说明结果没有问题。：<code>scrapy check f1</code></p></li>
<li><p><strong>crawl</strong> 运行一个爬虫文件。：<code>scrapy crawl f1</code> 或者 <code>scrapy crawl f1 --nolog</code></p></li>
<li><p><strong>list</strong> 列出当前爬虫项目下所有的爬虫文件： <code>scrapy list</code></p></li>
<li><p><strong>edit</strong> 使用编辑器打开爬虫文件 （Windows上似乎有问题，Linux上没有问题）：<code>scrapy edit f1</code></p></li>
</ul>


<hr />
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Learning Python 013 按行读取文件（逐行读取） --- 按行写入文件（逐行写入） --- 实战：从字幕文件中提取字幕内容]]></title>
    <link href="http://aobojaing.github.io/blog/2016/11/18/Learning-python-013-read-and-write-one-line/"/>
    <updated>2016-11-18T04:41:50+08:00</updated>
    <id>http://aobojaing.github.io/blog/2016/11/18/Learning-python-013-read-and-write-one-line</id>
    <content type="html"><![CDATA[<hr />

<p>使用的开发集成环境：PyCharm 2016.1.4
使用的Python的版本：python 2.7.10</p>

<hr />

<h2>知识点：Python 按行读取文件</h2>

<h3>读取整个文件的内容</h3>

<pre><code class="python">f = open('filename.txt', 'r')
text = f.read()
f.close()

print text
</code></pre>

<h3>读取一行的内容（按行读文件的内容）</h3>

<p>参考网站：</p>

<p>Python逐行读取文件内容
<a href="http://www.cnblogs.com/sysuoyj/archive/2012/03/14/2395789.html">http://www.cnblogs.com/sysuoyj/archive/2012/03/14/2395789.html</a></p>

<pre><code class="python">f = open('filename.txt', 'r')

while 1:
    line = f.readline()
    if not line:
        break
    print text
    pass

f.close()
</code></pre>

<p>缺点：它依赖于前后文的关系，所以不能获取指定行的内容。</p>

<h3>读取一行的内容 和 行号</h3>

<p>参考网站：</p>

<p>python读取文件同时输出行号和内容
<a href="http://outofmemory.cn/code-snippet/3222/python-duqu-file-tongshi-output-xinghao-content">http://outofmemory.cn/code-snippet/3222/python-duqu-file-tongshi-output-xinghao-content</a></p>

<pre><code class="python">f = open('filename.txt', 'r')
for (num,value) in enumerate(f):
    print "line number", num, "is:", value
f.close()
</code></pre>

<p>缺点：虽然这段代码可以获取到指定行的内容，但是，使用<code>enumerate()</code>函数获取指定行内容的代价是：需要对所有<strong>行</strong>依次进行编号，可想运算量之大。</p>

<h3>读取指定行的内容</h3>

<p>参考网站：</p>

<p>Python 从指定行读取数据
<a href="http://blog.163.com/xiaowei_090513/blog/static/11771835920140251257802">http://blog.163.com/xiaowei_090513/blog/static/11771835920140251257802</a></p>

<pre><code class="python">import linecache

num = 20
linecache.getline('filename.txt', num)
</code></pre>

<p>优点：读取的是缓存内的数据，速度快，并且代码简单。</p>

<hr />

<h2>知识点：Python 按行写入文件</h2>

<p>参考网站：</p>

<p>Python文件读写
<a href="http://cxymrzero.github.io/blog/2015/03/19/python-file/">http://cxymrzero.github.io/blog/2015/03/19/python-file/</a>
Python读写文件
<a href="http://blog.csdn.net/adupt/article/details/4435615">http://blog.csdn.net/adupt/article/details/4435615</a></p>

<h3>写一行（新建文件、替换现有文件）</h3>

<pre><code class="python">f = open('namefile.txt', 'w')
f.write('the first line: hello world in the new file.\n')
f.write('the sencond line: ')
f.write('this is also the second line.\n')
f.close()

f = open('namefile.txt', 'r')
text = f.read()
f.close()
print text
</code></pre>

<p>执行输出：</p>

<pre><code>the first line: hello world in the new file.
the sencond line: this is also the second line.
</code></pre>

<p><code>f = open('namefile.txt', 'w')</code>这段代码的执行：如果有这个<code>namefile.txt</code>文件，那就将现有的<code>namefile.txt</code>文件里面的内容全部自动清空；如果没有这个<code>namefile.txt</code>文件，那就自动新建一个<code>namefile.txt</code>文件。</p>

<h3>写一行（在原有文件后面追加写入）</h3>

<pre><code class="python">f = open('namefile.txt', 'w+')
f.write('the last line: hello world in the new file.\n')
f.close()

f = open('namefile.txt', 'r')
text = f.read()
f.close()
print text
</code></pre>

<p>输出：</p>

<pre><code>the first line: hello world in the new file.
the sencond line: this is also the second line.
the last line: hello world in the new file.
</code></pre>

<hr />

<h2>实战：从字幕文件中提取字幕内容</h2>

<p>github源代码网址：<a href="https://github.com/AoboJaing/youtube-srt-to-txt">https://github.com/AoboJaing/youtube-srt-to-txt</a></p>

<h3>如何获取字幕文件</h3>

<p>参考网站：<a href="https://www.youtube.com/watch?v=d9ctCc2AXCw">https://www.youtube.com/watch?v=d9ctCc2AXCw</a></p>

<p>视频网站： <a href="https://www.youtube.com/">https://www.youtube.com/</a></p>

<p>字幕提取网站：<a href="http://mo.dbxdb.com/setting.html">http://mo.dbxdb.com/setting.html</a></p>

<h3>设计思路</h3>

<p>输入是英文字幕文件和中文字幕文件，输出是英文配中文的txt文件</p>

<p>英文字幕文件</p>

<p><img src="/images/2016-11-18-Learning-python-013-read-and-write-one-line/1479414135007.png" alt="Alt text" /></p>

<p>中文字幕文件</p>

<p><img src="/images/2016-11-18-Learning-python-013-read-and-write-one-line/1479414161811.png" alt="Alt text" /></p>

<p>输出：英文配中文的txt文件</p>

<p><img src="/images/2016-11-18-Learning-python-013-read-and-write-one-line/1479414198952.png" alt="Alt text" /></p>

<h3>代码</h3>

<pre><code class="python"># coding : utf-8

import linecache

file_srt = open('Servos - working principle and homemade types.srt', 'r')
file_txt = open('newfile.txt', 'w')

cout=1
for (num, value) in enumerate(file_srt):
    if cout == 5:
        cout = 1
    if cout == 3:
        file_txt.write(linecache.getline('en-Servos - working principle and homemade types.srt', num+1))
        file_txt.write(value)
    cout += 1

file_srt.close()
file_txt.close()
</code></pre>

<h3>运行输出</h3>

<pre><code>Youtube subtitles download by mo.dbxdb.com 
Youtube subtitles download by mo.dbxdb.com 
In this video I would like to explain the functionality of servos and how to convert conventional DC motors into homebuilt servos.
在这部影片中，我想解释一下舵机以及如何传统的直流电动机转换成自制伺服系统的功能。 
A servo is a device that produces motion accordant to a command signal from a control system.
伺服是，从一个控制系统产生运动一致来的命令信号的装置。 
Usually an electric motor is used to create a mechanical force and the servomechanism rotates at a velocity that approximates the command signal.
一般的电动机被用来创建一个机械力和伺服机构在近似于指令信号的速度旋转。 
...
...
</code></pre>
]]></content>
  </entry>
  
</feed>
