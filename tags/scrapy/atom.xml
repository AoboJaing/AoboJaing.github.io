<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: scrapy | AoboSir 博客]]></title>
  <link href="http://aobojaing.github.io/tags/scrapy/atom.xml" rel="self"/>
  <link href="http://aobojaing.github.io/"/>
  <updated>2016-11-26T18:02:12+08:00</updated>
  <id>http://aobojaing.github.io/</id>
  <author>
    <name><![CDATA[Aobo Jaing]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Python --- Scrapy 命令]]></title>
    <link href="http://aobojaing.github.io/blog/2016/11/26/python-Scrapy-command/"/>
    <updated>2016-11-26T06:39:53+08:00</updated>
    <id>http://aobojaing.github.io/blog/2016/11/26/python-Scrapy-command</id>
    <content type="html"><![CDATA[<hr />

<p>Scrapy 命令 分为两种：<strong>全局命令</strong> 和 <strong>项目命令</strong>。</p>

<p>全局命令：在哪里都能使用。</p>

<p>项目命令：必须在爬虫项目里面才能使用。</p>

<h2>全局命令</h2>

<pre><code>C:\Users\AOBO&gt;scrapy -h
Scrapy 1.2.1 - no active project

Usage:
  scrapy &lt;command&gt; [options] [args]

Available commands:
  bench         Run quick benchmark test
  commands
  fetch         Fetch a URL using the Scrapy downloader
  genspider     Generate new spider using pre-defined templates
  runspider     Run a self-contained spider (without creating a project)
  settings      Get settings values
  shell         Interactive scraping console
  startproject  Create new project
  version       Print Scrapy version
  view          Open URL in browser, as seen by Scrapy

  [ more ]      More commands available when run from project directory

Use "scrapy &lt;command&gt; -h" to see more info about a command
</code></pre>

<ul>
<li><strong>startproject</strong>：创建一个爬虫项目：<code>scrapy startproject demo</code>（<code>demo</code> 创建的爬虫项目的名字）</li>
<li><strong>runspider</strong> 运用单独一个爬虫文件：<code>scrapy runspider abc.py</code></li>
<li><strong>veiw</strong> 下载一个网页的源代码，并在默认的文本编辑器中打开这个源代码：<code>scrapy view http://www.aobossir.com/</code></li>
<li><strong>shell</strong> 进入交互终端，用于爬虫的调试（如果你不调试，那么就不常用）：<code>scrapy shell http://www.baidu.com --nolog</code>（<code>--nolog</code> 不显示日志信息）</li>
<li><strong>version</strong> 查看版本：（<code>scrapy version</code>）</li>
<li><strong>bench</strong> 测试本地硬件性能（工作原理：）：<code>scrapy bench</code> （如果遇到问题：解决问题:  <code>import win32api ImportError: DLL load failed</code>，到<a href="http://www.aobosir.com/blog/2016/11/26/solve-pywin32-import-win32api-ImportError-DLL-load-failed/">这里</a>查看解决办法。）</li>
</ul>


<hr />

<h2>项目命令</h2>

<p>（进入项目路径，才能看到项目命令）</p>

<pre><code>D:\BaiduYunDownload\first&gt;scrapy -h
Scrapy 1.2.1 - project: first

Usage:
  scrapy &lt;command&gt; [options] [args]

Available commands:
  bench         Run quick benchmark test
  check         Check spider contracts
  commands
  crawl         Run a spider
  edit          Edit spider
  fetch         Fetch a URL using the Scrapy downloader
  genspider     Generate new spider using pre-defined templates
  list          List available spiders
  parse         Parse URL (using its spider) and print the results
  runspider     Run a self-contained spider (without creating a project)
  settings      Get settings values
  shell         Interactive scraping console
  startproject  Create new project
  version       Print Scrapy version
  view          Open URL in browser, as seen by Scrapy

Use "scrapy &lt;command&gt; -h" to see more info about a command

D:\BaiduYunDownload\first&gt;
</code></pre>

<ul>
<li><strong>genspider</strong> 创建一个爬虫文件，我们在爬虫项目里面才能创建爬虫文件（这个命令用的非常多）（<strong>startproject</strong>：创建一个爬虫项目）。创建爬虫文件是按照以下模板来创建的，使用<code>scrapy genspider -l</code> 命令查看有哪些模板。</li>
</ul>


<pre><code>D:\BaiduYunDownload\first&gt;scrapy genspider -l
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed

D:\BaiduYunDownload\first&gt;
</code></pre>

<blockquote><p><code>basic</code> 基础
<code>crawl</code>自动爬虫
<code>csvfeed</code>用来处理csv文件
<code>xmlfeed</code>用来处理xml文件</p>

<p>按照<code>basic</code>模板创建一个名为<code>f1</code>的爬虫文件：<code>scrapy genspider -t basic f1</code> ，创建了一个<code>f1.py</code>文件。</p></blockquote>

<ul>
<li><p><strong>check</strong> 测试爬虫文件、或者说：检测一个爬虫，如果结果是：OK，那么说明结果没有问题。：<code>scrapy check f1</code></p></li>
<li><p><strong>crawl</strong> 运行一个爬虫文件。：<code>scrapy crawl f1</code> 或者 <code>scrapy crawl f1 --nolog</code></p></li>
<li><p><strong>list</strong> 列出当前爬虫项目下所有的爬虫文件： <code>scrapy list</code></p></li>
<li><p><strong>edit</strong> 使用编辑器打开爬虫文件 （Windows上似乎有问题，Linux上没有问题）：<code>scrapy edit f1</code></p></li>
</ul>


<hr />
]]></content>
  </entry>
  
</feed>
