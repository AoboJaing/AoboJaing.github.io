<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: 爬虫 | AoboSir 博客]]></title>
  <link href="http://aobojaing.github.io/tags/pa-chong/atom.xml" rel="self"/>
  <link href="http://aobojaing.github.io/"/>
  <updated>2017-02-18T22:20:36+08:00</updated>
  <id>http://aobojaing.github.io/</id>
  <author>
    <name><![CDATA[Aobo Jaing]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Python3 大型网络爬虫实战 004 — scrapy 大型静态商城网站爬虫项目编写及数据写入数据库实战 — 实战：爬取淘宝]]></title>
    <link href="http://aobojaing.github.io/blog/2016/12/26/python3-large-web-crawler-taobao-com-import-to-MySQL-database/"/>
    <updated>2016-12-26T00:56:25+08:00</updated>
    <id>http://aobojaing.github.io/blog/2016/12/26/python3-large-web-crawler-taobao-com-import-to-MySQL-database</id>
    <content type="html"><![CDATA[<hr />

<p>[TOC]</p>

<h2>开发环境</h2>

<ul>
<li>Python第三方库：lxml、Twisted、pywin32、scrapy</li>
<li>Python 版本：python-3.5.0-amd64</li>
<li>PyCharm软件版本：pycharm-professional-2016.1.4</li>
<li>电脑系统：Windows 10 64位</li>
</ul>


<p>如果你还没有搭建好开发环境，请到<a href="http://www.aobosir.com/blog/2016/11/26/python3-large-web-crawler-001-Build-development-environment/">这篇博客</a>。</p>

<hr />

<ul>
<li>本文中的源代码在github这里：<a href="https://github.com/AoboJaing/thirdDemo/">https://github.com/AoboJaing/thirdDemo/</a></li>
</ul>


<p>本篇博文的重点内容：</p>

<ul>
<li>有一些数据，在源码上找不到的，这个时候需要使用 &mdash; 抓包。</li>
<li>Python调用MySQL数据库</li>
</ul>


<hr />

<p>本爬虫项目的目的是：某个关键字在淘宝上搜索到的所有商品，获取所有商品的： 商品名字、商品链接、商品价格、商品的评论。</p>

<hr />

<h2>开始实战</h2>

<p>创建一个爬虫项目</p>

<pre><code>scrapy startproject thirdDemo
</code></pre>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480949909390.png" alt="Alt text" /></p>

<h2>设置防反爬机制（settings.py 文件）</h2>

<p>请参考这篇博客：<a href="http://www.aobosir.com/blog/2016/12/06/python3-large-web-crawler-scrapy-project-Anti-reptile-settings/">给 Scrapy 爬虫项目设置为防反爬</a>。</p>

<h2>分析网站</h2>

<ul>
<li>分析网页界面</li>
<li>分析网址结构</li>
<li>分析网页源代码</li>
</ul>


<p>1 . 分析网页界面：</p>

<p>我们在淘宝网的搜索栏里面搜索关键字，比如“小吃”。它一共会输出100页。</p>

<blockquote><p>可见：100页的搜索结果是淘宝的上限。（最多100页）</p></blockquote>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480951614377.png" alt="Alt text" /></p>

<p>2 . 分析网址结构：</p>

<p>当我们点击页面进行浏览时，我们发现不同的页面的网址有规律，并且下面是我们找到的规律：</p>

<ol>
<li>红色部分是一模一样的。</li>
<li>删除红色部分，将剩下的组成网址，一样可以正常的浏览原网页。</li>
<li><code>q=</code> 后面是“小吃”的编码形式。</li>
<li><code>s=</code> 后面的数值等于 <code>44*(当前页面-1)</code></li>
</ol>


<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480951515511.png" alt="Alt text" /></p>

<h1>开始写爬虫程序（taobao.py 文件）</h1>

<h2>创建一个爬虫文件（taobao.py 文件）</h2>

<pre><code>cd thirdDemo
scrapy genspider -t basic taobao taobao.com
</code></pre>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480949962287.png" alt="Alt text" /></p>

<p>使用PyCharm软件开发，使用PyCharm软件打开 <strong>thirdDemo</strong>项目。</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480950062203.png" alt="Alt text" /></p>

<h2>添加需要使用的存储容器对象（items.py文件）</h2>

<p>先到 <code>items.py</code> 文件里面的<code>ThirddemoItem()</code>函数里面创建存储用到容器（类的实例化对象）</p>

<pre><code class="python">class ThirddemoItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    title = scrapy.Field()
    link = scrapy.Field()
    price = scrapy.Field()
    comment = scrapy.Field()
    pass
</code></pre>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480950185425.png" alt="Alt text" /></p>

<h2>得到搜索关键字对应的所有搜索页面（taobao.py文件）</h2>

<p>在回调函数<code>parse()</code>中，建立一个变量(<code>key</code>)来存储关键词（<code>零食</code>）。然后在使用一个<code>for</code>循环来爬取所有的网页。然后使用<code>scrapy.http</code>里面的<code>Request</code> 来在<code>parse()</code>函数返回（返回一个生成器（yield））一个网页源代码：</p>

<pre><code class="python"># -*- coding: utf-8 -*-
import scrapy
from scrapy.http import Request

class TaobaoSpider(scrapy.Spider):
    name = "taobao"
    allowed_domains = ["taobao.com"]
    start_urls = ['http://taobao.com/']

    def parse(self, response):
        key = '小吃'
        for i in range(0, 2):
            url = 'https://s.taobao.com/search?q=' + str(key) + '&amp;s=' + str(44*i)
            print(url)
            yield Request(url=url, callback=self.page)
        pass

    def page(self, response):
        pass
</code></pre>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480952150312.png" alt="Alt text" /></p>

<p>（注意：我们上面通过观察网页已经知道了，搜索得到的页面有100页，但是我们现在是测试阶段，不爬这么多页，上面的代码我们只爬了2页）</p>

<hr />

<blockquote><p>运行一下：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480952373465.png" alt="Alt text" /></p>

<p>程序没有问题。</p></blockquote>

<hr />

<h2>得到所有商品的id</h2>

<p>我们现在的目标是得到搜索页面中所有商品的链接。</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480958203181.png" alt="Alt text" /></p>

<p>现在，我们观察搜索页面的源代码，我们找这些商品链接的规律，能否通过什么商品<code>id</code>之类的信息，然后通过商品<code>id</code>来构造出商品的链接网址。</p>

<p>幸运的是：确实可以这么做。</p>

<p>我发现，不管是搜索的商品，不管是在淘宝里、天猫里、天猫超市里，商品的链接网址都可以用下面的格式去构造：</p>

<pre><code>https://item.taobao.com/item.htm?id=商品的id
</code></pre>

<p>所以，现在我们要做的是：先提取商品的id：（使用正则表达式）</p>

<p>对搜索结果的网页随便一个地方右键：<strong>查看网页源代码(V)</strong>：</p>

<blockquote><p>（我发现：通过在浏览器中按<strong>F12</strong> 和 右键来 <strong>查看网页源代码</strong> 这两种查看源代码得到的源代码不一样，后者得到的源代码和爬虫爬取的源代码一致，而前者和爬虫爬取的不一致。）</p>

<p> 所有我们不能使用<strong>Xpath</strong>表达式来用过标签获取商品id了。只能使用<strong>正则表达式</strong>了。</p>

<p>我想可能的原因是：搜索页面可能是动态构造出来的，所以使用<strong>Xpath</strong>表达式是不能对这种网址的源码进行提取信息的。（我瞎想的，不知道是否正确。不过事实其实是：使用<strong>Xpath</strong>表达式是提取不了有效信息的。）</p></blockquote>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480965022146.png" alt="Alt text" /></p>

<p>然后随便点击进入一个商品的链接网页，在这个网页的网址里面就可以找到这个商品的id：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480965129366.png" alt="Alt text" /></p>

<p>然后在刚刚打开的源代码中，查找这个id:</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480965212130.png" alt="Alt text" /></p>

<p>我们通过观察发现，使用<code>"nid":"</code>就可以找到这个搜索结果页面里面所有商品的id：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480965247629.png" alt="Alt text" /></p>

<blockquote><p>这个页面里面一共是36个商品，没错。</p>

<p><strong>Q：</strong> 你可能发现了，这个搜索网页里面搜索到结果是48个商品，我们得到的是36个，是不是少了？</p>

<p><strong>A：</strong> 没有少，这就是淘宝的营销策略。一个搜索页面一共有48个商品，但是其中有10多个商品是重复的！其中甚至有个商品在这个搜索页面中重复出现了三次，不信，你可以仔细的找找。</p></blockquote>

<p>所以，商品的id可以使用下面的<strong>正则表达式</strong>获取：</p>

<pre><code>'"nid":"(.*?)"'
</code></pre>

<p>我们在<code>page()</code>方法中得到爬取到的网页源代码的 <code>body</code> 标签里面的所有信息：</p>

<blockquote><p>先声明一点：</p>

<p> 爬取到的网页源代码是：以网页源代码中指定的编码方式编码得到的bytes信息。</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480954729660.png" alt="Alt text" /></p></blockquote>

<p>我们需要得到对应的解码信息：</p>

<p>参考网站：<a href="http://www.runoob.com/python3/python3-string-decode.html">Python3 bytes.decode()方法</a></p>

<pre><code class="python">        body = response.body.decode('utf-8')
</code></pre>

<blockquote><p><code>response.body</code> 它默认是二进制格式，所以我们在使用它之前要给它解码：<code>decode('utf-8')</code>，为了避免出错，我给它传第二个参数：<code>ignore</code>。</p></blockquote>

<p><code>page()</code>函数中的代码现在是下面这个样子的：</p>

<pre><code class="python">    def page(self, response):
        body = response.body.decode('utf-8','ignore')
        pattam_id = '"nid":"(.*?)"'
        all_id = re.compile(pattam_id).findall(body)
        print(all_id)
        pass
</code></pre>

<blockquote><p>运行试试看：</p>

<p> <img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480965691671.png" alt="Alt text" /></p></blockquote>

<hr />

<h2>得到所有商品的链接网址</h2>

<p>现在得到了所有商品的ip，现在通过这些ip，构造出所有商品的网址。得到了链接后，就可以去爬这个网页的源代码了：（下面的代码中，在<code>next()</code>方法中将商品的网页网址打印了出来）</p>

<pre><code class="python">import re
</code></pre>

<pre><code class="python">    def page(self, response):
        body = response.body.decode('utf-8','ignore')
        pattam_id = '"nid":"(.*?)"'
        all_id = re.compile(pattam_id).findall(body)
        # print(all_id)
        for i in range(0, len(all_id)):
            this_id = all_id[i]
            url = 'https://item.taobao.com/item.htm?id=' + str(this_id)
            yield Request(url=url, callback=self.next)
            pass
        pass

    def next(self, response):
        print(response.url)
        pass
</code></pre>

<blockquote><p>运行试试看：（自动的将网址调整到正确的网址上。比如<strong>天猫</strong>或者<strong>天猫超市</strong>之类的子域名）</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480966453270.png" alt="Alt text" /></p></blockquote>

<hr />

<h1>获取商品的具体信息（taobao.py 文件）</h1>

<h2>获取商品的名字</h2>

<p>现在在<code>next()</code>回调函数中实例化一个开始时在<code>items.py</code>文件里面创建的项目的存储容器对象。然后，我们就可以直接使用它了。</p>

<p>所以现在在 <code>taobao.py</code> 文件的上面添加这个文件：</p>

<pre><code class="python">from thirdDemo.items import ThirddemoItem
</code></pre>

<p>现在我们要得到商品的标题。</p>

<blockquote><p>我们尽量从源代码中的信息提取，如果源代码中没有的信息，我们在使用抓包的凡是提取。</p></blockquote>

<p>标题是可以直接在源代码中提取的：（观察网页源代码，直接中<code>Xpath</code>表达式）</p>

<p>天猫或者天猫超市的商品的标题可以使用下面的<strong>Xpath</strong>表达式提取：</p>

<pre><code>title = response.xpath("//div[@class='tb-detail-hd']/h1/text()").extract()
</code></pre>

<p>淘宝的商品的标题可以使用下面的<strong>Xpath</strong>表达式提取：</p>

<pre><code>title = response.xpath("//h3[@class='tb-main-title']/@data-title").extract()
</code></pre>

<p>所以，这里提取标题，我们需要一个判断语句，判断这个商品的网址链接是天猫的还是淘宝的。</p>

<p>伪码如下：</p>

<pre><code>if 不是淘宝的网址:
    title = response.xpath("//div[@class='tb-detail-hd']/h1/text()").extract() # 天猫或者天猫超市
else:
    title = response.xpath("//h3[@class='tb-main-title']/@data-title").extract() # 淘宝
</code></pre>

<p>我们的判断标准就是商品网址的子域名。子域名大致一共有三种：<code>detail.tmall</code>（天猫）、<code>chaoshi.detail.tmall</code>（天猫超市）、<code>item.taobao</code>（淘宝）</p>

<pre><code class="python">    def next(self, response):
        # print(response.url)
        url = response.url
        pattam_url = 'https://(.*?).com'
        subdomain = re.compile(pattam_url).findall(url)
        # print(subdomain)
        if subdomain[0] != 'item.taobao':
            title = response.xpath("//div[@class='tb-detail-hd']/h1/text()").extract()
            pass
        else:
            title = response.xpath("//h3[@class='tb-main-title']/@data-title").extract()
            pass
        self.num = self.num + 1;
        print(title)
        pass
</code></pre>

<blockquote><p>运行试试看：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480971492185.png" alt="Alt text" /></p>

<p>有的时候，偶尔会得到几个 <code>[]</code>，这是因为，你爬的太快的，淘宝的服务器没有同意你爬取这个商品的网页。（所以提高防反爬机制，效果会好一些。）</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480971883848.png" alt="Alt text" /></p></blockquote>

<h2>获取商品的链接网址（taobao.py 文件）</h2>

<p>（直接得到）</p>

<pre><code class="python">        item['link'] = response.url
</code></pre>

<h2>获取商品的价格信息（原价）（taobao.py 文件）</h2>

<blockquote><p>正常的价格可以在商品网页的源代码里面获取，但是淘宝价（促销价）在商品源代码里面没有，这时就需要通过抓包来获取。</p></blockquote>

<p>淘宝：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480973755825.png" alt="Alt text" /></p>

<p>天猫：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480973520884.png" alt="Alt text" /></p>

<p>我们先获取正常的价格。这里也需要分淘宝和天猫，它们获取正常价格的<strong>Xpath</strong>表达式或者<strong>正则表达式</strong>不同。</p>

<blockquote><p>注意：这里总结表达式，通过对商品页面右键 -> <strong>查看网页源代码</strong> 的方式查看源代码。</p></blockquote>

<pre><code class="python">        if subdomain[0] != 'item.taobao':
            pattam_price = '"defaultItemPrice":"(.*?)"'
            price = re.compile(pattam_price).findall(response.body.decode('utf-8', 'ignore')) # 天猫
            pass
        else:
            price = response.xpath("//em[@class = 'tb-rmb-num']/text()").extract() # 淘宝
            pass
        print(price)
</code></pre>

<h2>提取商品的累计评论数量：（使用抓包的方式）（taobao.py 文件）</h2>

<p>淘宝：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480973770234.png" alt="Alt text" /></p>

<p>天猫：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480973787179.png" alt="Alt text" /></p>

<blockquote><p>可以使用 ： <strong>Fiddler4抓包软件</strong> 或者 浏览器按<strong>F12->Network->Name->Response</strong>查看抓包信息</p></blockquote>

<p>这里，我通过浏览器进行抓包，找到了评论数所在的包：（一个一个的找）</p>

<blockquote><p>淘宝：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480990614603.png" alt="Alt text" /></p>

<p>观察这个包的网址：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480990661164.png" alt="Alt text" /></p>

<p>这个网址，我们可以在浏览器中复制，再访问以下：（是可以正常访问的）</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480990914346.png" alt="Alt text" /></p>

<pre><code>https://rate.taobao.com/detailCommon.htm?auctionNumId=533237707421&amp;userNumId=1990097437&amp;ua=097UW5TcyMNYQwiAiwQRHhBfEF8QXtHcklnMWc%3D%7CUm5Ockt%2BQ3dDfkB8R35Eey0%3D%7CU2xMHDJ7G2AHYg8hAS8XKQcnCVU0Uj5ZJ11zJXM%3D%7CVGhXd1llXGlUYFRpV2tQaVFvWGVHekV8RHtBf0Z%2FQXRKdUx1T3VOYDY%3D%7CVWldfS0TMw8xBD8fIAAubQslcyU%3D%7CVmJCbEIU%7CV2lJGSQEORklGCMYOAI%2FADkZJREuEzMPMgc6GiYSLRAwDDEJNGI0%7CWGFcYUF8XGNDf0Z6WmRcZkZ8R2dZDw%3D%3D&amp;callback=json_tbc_rate_summary
</code></pre>

<p>我发现上面的这个网址可以缩减为：</p>

<pre><code>https://rate.taobao.com/detailCommon.htm?auctionNumId=533237707421
</code></pre>

<pre><code>https://rate.taobao.com/detailCount.do?_ksTS=1480993623725_99&amp;callback=jsonp100&amp;itemId=533237707421
</code></pre>

<p>而这个<code>533237707421</code>就是商品的id。好了，找到这个网址的规律，现在可以> 手动构造这个评论数的网址了：</p>

<p>天猫：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480993211429.png" alt="Alt text" /></p>

<pre><code>https://dsr-rate.tmall.com/list_dsr_info.htm?itemId=35338957824&amp;spuId=235704813&amp;sellerId=628189716&amp;_ksTS=1480992656788_203&amp;callback=jsonp204
</code></pre>

<p>可以缩减为：</p>

<pre><code>https://dsr-rate.tmall.com/list_dsr_info.htm?itemId=35338957824
</code></pre></blockquote>

<p>最后，我们发现：不管是淘宝还是天猫，都可以使用下面这个构造方式来得到含有正确评论数量的网址：</p>

<pre><code>https://dsr-rate.tmall.com/list_dsr_info.htm?itemId=商品id
</code></pre>

<blockquote><p>注意：使用<code>https://rate.taobao.com/detailCommon.htm?auctionNumId=商品id</code> 这种网址也可以，但是在对天猫商品得到评价数量和网页里面显示的不同。所以我们不使用这个构造方法。</p>

<p> <img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480994835425.png" alt="Alt text" /></p>

<p>  <img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480994874358.png" alt="Alt text" /></p></blockquote>

<p>所以通过商品id就可以得到含有评论数量信息的包的网址。现在在<code>next()</code>方法中需要通过商品的URL获取商品的id。</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480995644556.png" alt="Alt text" /></p>

<p>我们从上面的图中看到：天猫和淘宝的网址不同，所以，从网址中获取商品id的正则表达式也就不同。下面的代码的功能就是从商品的url中提取商品id：</p>

<pre><code class="python">        # 获取商品的id（用于构造商品评论数量的抓包网址）
        if subdomain[0] != 'item.taobao':  # 如果不属于淘宝子域名，执行if语句里面的代码
            pattam_id = 'id=(.*?)&amp;'
            this_id = re.compile(pattam_id).findall(url)[0]
            pass
        else:
            # 这种情况是不能使用正则表达式的，正则表达式不能获取字符串最末端的字符串
            pattam_id = 'id=(.*?)$'
            this_id = re.compile(pattam_id).findall(url)[0]
            pass
        print(this_id)
</code></pre>

<blockquote><p>注意：<code>$</code> : 在<strong>正则表达式</strong>里面的作用是：匹配字符串末尾。</p>

<p>举例：当<code>url = 'https://item.taobao.com/item.htm?id=535023141744'</code> 时，这是一个淘宝网站里面的一个商品，现在我们想得到这个网址里面的商品id。</p>

<p>如果你把正则表达式写成这个样子：<code>pattam_id = 'id=(.*?)'</code>，是匹配不到结果的（商品id）。</p>

<p><strong>正则表达式是通过字符串上下文来匹配你需要的信息的，如果只有“上文”，没有“下文”时，对于使用正则表达式匹配字符串末端字符串，需要在正则表达式中使用<code>$</code>。</strong></p></blockquote>

<hr />

<blockquote><p>运行试试看，一切都在掌控之中。</p></blockquote>

<h2>构造具有评论数量信息的包的网址，并获取商品的评论数量</h2>

<p>得到目标抓包网址，获取它的源代码，然后提取评论数量：</p>

<pre><code class="python">import urllib
</code></pre>

<pre><code class="python">        # 构造具有评论数量信息的包的网址
        comment_url = 'https://dsr-rate.tmall.com/list_dsr_info.htm?itemId=' + str(this_id)

        # 这个获取网址源代码的代码永远也不会出现错误，因为这个URL的问题，就算URL是错误的，也可以获取到对应错误网址的源代码。
        # 所以不需要使用 try 和 except urllib.URLError as e 来包装。
        comment_data = urllib.request.urlopen(comment_url).read().decode('utf-8', 'ignore')
        pattam_comment = '"rateTotal":(.*?),"'
        comment = re.compile(pattam_comment).findall(comment_data)
        # print(comment)
        item['comment'] = comment
</code></pre>

<p>现在返回<code>item</code>对象：</p>

<pre><code class="python">        yield item
</code></pre>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480999254433.png" alt="Alt text" /></p>

<hr />

<p>现在，我们就可以在<code>pipline.py</code>文件里面来对我们得到的这些商品数据进行一些操作了，比如打印到终端或者保存到数据库中。</p>

<p>但在这之前，我们需要设置一下<code>settings.py</code>文件，将下面的代码的注释去掉：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480999396253.png" alt="Alt text" /></p>

<hr />

<p>在<code>pipline.py</code>文件中对<code>taobao.py</code>爬虫文件返回的<code>item</code>对象进行处理（比如打印到终端，或者保存到数据库中）</p>

<p>将得到的信息打印到终端中：</p>

<pre><code class="python">class ThirddemoPipeline(object):
    def process_item(self, item, spider):
        title = item['title'][0]
        link = item['link']
        price = item['price'][0]
        comment = item['comment'][0]
        print('商品名字', title)
        print('商品链接', link)
        print('商品正常价格', price)
        print('商品评论数量', comment)
        print('------------------------------\n')
        return item
</code></pre>

<blockquote><p>运行试试看：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480999663330.png" alt="Alt text" /></p></blockquote>

<hr />

<p>下面是将得到的信息保存到数据库中的操作：</p>

<p>第一件事情就是 启动数据库，启动数据库的代码一般我们是将它写到默认的<code>__init__(self)</code>函数中，这个方法就是最开始做的事情。</p>

<p>要想连接到数据库，首先要有数据库：使用MySQL数据库</p>

<p>在python上要想使用MySqL数据库，需要先安装<code>pymysql</code>库这个模块。</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480855800994.png" alt="Alt text" /></p>

<p>有打开数据库的函数，就要有关闭数据库的方法。</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480855810885.png" alt="Alt text" /></p>

<p>现在，我们在<code>process()</code>函数中处理数据，将数据插入到数据库里面。并且加一个异常处理，因为我不希望程序运行的时候会出现错误而终止，并且我也不想</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-taobao-com-import-to-MySQL-database/1480856115248.png" alt="Alt text" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python3 大型网络爬虫实战 003 — scrapy 大型静态图片网站爬虫项目实战 — 实战：爬取 169美女图片网 高清图片]]></title>
    <link href="http://aobojaing.github.io/blog/2016/12/26/python3-large-web-crawler-169bb-com-HD-beautiful-pictures/"/>
    <updated>2016-12-26T00:14:57+08:00</updated>
    <id>http://aobojaing.github.io/blog/2016/12/26/python3-large-web-crawler-169bb-com-HD-beautiful-pictures</id>
    <content type="html"><![CDATA[<hr />

<p>[TOC]</p>

<h2>开发环境</h2>

<ul>
<li>Python第三方库：lxml、Twisted、pywin32、scrapy</li>
<li>Python 版本：python-3.5.0-amd64</li>
<li>PyCharm软件版本：pycharm-professional-2016.1.4</li>
<li>电脑系统：Windows 10 64位</li>
</ul>


<p>如果你还没有搭建好开发环境，请到<a href="http://www.aobosir.com/blog/2016/11/26/python3-large-web-crawler-001-Build-development-environment/">这篇博客</a>。</p>

<hr />

<ul>
<li>本篇博客源代码GitHub里：<a href="https://github.com/AoboJaing/secondDemo/">这里</a>。</li>
</ul>


<p>这一篇博客的目的是爬取 <a href="http://www.169bb.com/">169美女图片网</a> 里面的所有的“<a href="http://www.169bb.com/xiyangmeinv/">西洋美女</a>”的高清图片。</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480187579013.png" alt="Alt text" /></p>

<hr />

<p>爬虫程序设计思路：</p>

<p>1 . 先得到 <a href="http://www.169bb.com/xiyangmeinv/">http://www.169bb.com/xiyangmeinv/</a> 页面里面所有的照片后面对应的URL网页链接（<a href="http://www.169bb.com/xiyangmeinv/2016/1123/37380.html">如</a>）。</p>

<p>2 . 接着在得到的URL链接网页里面得到里面所有高清图片的下载地址，进行下载。</p>

<p>3 . 得到所有 “西洋美女” 网页的页数。</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480187965472.png" alt="Alt text" /></p>

<hr />

<h2>观察网页 和 网页源代码</h2>

<p>1 . 打开 169美女图片网：<a href="http://www.169bb.com/">http://www.169bb.com/</a></p>

<p>2 . 我们的目的是爬取这个站点里面所有 “西洋美女” 的高清图片。所以点击进入“西洋美女” 标签里。（<a href="http://www.169bb.com/xiyangmeinv/%EF%BC%89">http://www.169bb.com/xiyangmeinv/%EF%BC%89</a></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480188171424.png" alt="Alt text" /></p>

<p>3 . 观察这个页面，在页面最下面，显示了，当前一共311页。</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480188308560.png" alt="Alt text" /></p>

<p>4 . 我们再来观察每页的网址有什么共同点，我们发现：</p>

<ul>
<li>第2页的网站是：<a href="http://www.169bb.com/xiyangmeinv/list_4_2.html">http://www.169bb.com/xiyangmeinv/list_4_2.html</a></li>
<li>第3页的网址是：<a href="http://www.169bb.com/xiyangmeinv/list_4_3.html">http://www.169bb.com/xiyangmeinv/list_4_3.html</a></li>
<li>第1页的网站是：<a href="http://www.169bb.com/xiyangmeinv/list_4_1.html">http://www.169bb.com/xiyangmeinv/list_4_1.html</a></li>
</ul>


<p>这样每页的网址是有规律的，按照这个规律，我们可以推测出“西洋美女” 的第120页的网址就应该是：<a href="http://www.169bb.com/xiyangmeinv/list_4_120.html">http://www.169bb.com/xiyangmeinv/list_4_120.html</a>
。事实的确是这样的。好。</p>

<p>5 . 现在，我们随便点击一个图片，进去看看这个美女的高清图片集。</p>

<p>里面都是高清的图片，并且有很多，并且，不止一页。就我随机点击的这个美女的链接就有11页，并且一页里面有5张左右的高清图片。</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480188754820.png" alt="Alt text" /></p>

<p>6 . 并且它的每页的网址也是有规律的。</p>

<ul>
<li>第2页是：<a href="http://www.169bb.com/xiyangmeinv/2016/1117/37333_2.html">http://www.169bb.com/xiyangmeinv/2016/1117/37333_2.html</a></li>
<li>第3页是：<a href="http://www.169bb.com/xiyangmeinv/2016/1117/37333_3.html">http://www.169bb.com/xiyangmeinv/2016/1117/37333_3.html</a></li>
<li>第1页是：<a href="http://www.169bb.com/xiyangmeinv/2016/1117/37333.html">http://www.169bb.com/xiyangmeinv/2016/1117/37333.html</a></li>
<li>&hellip;</li>
</ul>


<p>但是有的美女的网址里只有一页，比如这个：<a href="http://www.169bb.com/xiyangmeinv/2016/0103/5974.html">http://www.169bb.com/xiyangmeinv/2016/0103/5974.html</a></p>

<hr />

<p>好了，现在这个目标网页，我们已经分析完了。现在就可以编程。</p>

<hr />

<h2>写程序</h2>

<p>源代码GitHub里：<a href="https://github.com/AoboJaing/secondDemo/">这里</a>。</p>

<p>接下来我们为大家讲解大型图片爬虫项目编写实战。</p>

<p><strong>Step 1 . </strong></p>

<p>创建一个Scrapy爬虫项目：</p>

<pre><code>scrapy startproject secondDemo
</code></pre>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480190424967.png" alt="Alt text" /></p>

<p>创建一个scrapy爬虫文件，我们就在这个文件里面写爬取目标网站里面图片的爬虫程序。</p>

<pre><code>cd secondDemo
scrapy genspider -t basic pic_169bb 169bb.com
</code></pre>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480190488955.png" alt="Alt text" /></p>

<hr />

<p>用 <strong>PyCharm</strong> 软件打开刚刚创建的 <code>secondDemo</code> 工程。</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480190609159.png" alt="Alt text" /></p>

<p><strong>Step 2 . </strong> 在 <code>items.py</code> 文件里面的<code>SeconddemoItem()</code>函数里面创建一个对象，这个对象在其他的文件里面会使用到。</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480191956323.png" alt="Alt text" /></p>

<p><strong>Step 3 . </strong> 现在开始爬虫的编写。进入<code>pic_169bb.py</code>文件。</p>

<p>爬虫（<code>pic_169bb.py</code>文件）会自动的先爬首页（<a href="http://169bb.com/">169bb.com</a>），爬完首页之后，会自动的进入<code>parse()</code>回调函数。</p>

<p>这个回调函数中，我们需要写些东西。</p>

<p>先获取所有栏目的名字和地址。</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480192407198.png" alt="Alt text" /></p>

<p>查看源代码：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480192613372.png" alt="Alt text" /></p>

<p>在<code>pic_169bb.py</code> 文件中的 <code>parse()</code>回调函数中添加下面的代码：</p>

<pre><code class="python">        urldata = response.xpath("/html/body/div[@class='header']/div[@class='hd_nav']/div[@class='w1000']//a/@href").extract()
        print(urldata)
</code></pre>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480196844577.png" alt="Alt text" /></p>

<p>现在运行一下，输出：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480196997090.png" alt="Alt text" /></p>

<hr />

<p>继续进行下一次的爬取：</p>

<p>先导入一个模块</p>

<pre><code class="python">from scrapy.http import Request
</code></pre>

<hr />

<p>爬取子栏目的第一页，即西洋美女网址的第一页。</p>

<pre><code class="python">        xiyangurldata = urldata[4]  # 获取西洋美女首页网址
        print(xiyangurldata)
        yield Request(url=xiyangurldata, callback=self.next)

    def next(self, response):
        pass
</code></pre>

<hr />

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480198132160.png" alt="Alt text" /></p>

<pre><code class="python">    def next(self, response):
        page_title_list = response.xpath("/html/body//div[@class='w1000 box03']/ul[@class='product01']//li/a/@alt").extract()
        print(page_title_list)
        page_url_list = response.xpath("/html/body//div[@class='w1000 box03']/ul[@class='product01']//li/a/@href").extract()
        print(page_url_list)
        pass
</code></pre>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480200182166.png" alt="Alt text" /></p>

<p>运行输出一下：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480200281135.png" alt="Alt text" /></p>

<hr />

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480200630246.png" alt="Alt text" /></p>

<pre><code class="python">        page_num = response.xpath("//span[@class='pageinfo']//strong/text()").extract()[0] # 得到西洋美女总页数
        print(page_num)
        print(response.url)
        for i in range(1, int(page_num)+1):
            page_url = response.url + 'list_4_'+ str(i) + '.html' # 得到西洋美女每一个页面的网址
            print(page_url)
        pass
</code></pre>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480201913887.png" alt="Alt text" /></p>

<p>运行输出一下：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480201965248.png" alt="Alt text" /></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480201944373.png" alt="Alt text" /></p>

<hr />

<p>继续下一次：</p>

<pre><code class="python">        for i in range(1, int(page_num)+1):
            page_url = response.url + 'list_4_'+ str(i) + '.html' # 得到西洋美女每一个页面的网址
            print(page_url)
            yield Request(url=page_url, callback=self.next2)
        pass

    def next2(self, response):
        pass
</code></pre>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480202151141.png" alt="Alt text" /></p>

<hr />

<p>现在获取每一个美女的网页网址：</p>

<pre><code class="python">    def next2(self, response):
        page_title_list = response.xpath("/html/body//div[@class='w1000 box03']/ul[@class='product01']//li/a/@alt").extract()
        # print(page_title_list)
        page_url_list = response.xpath("/html/body//div[@class='w1000 box03']/ul[@class='product01']//li/a/@href").extract()
        # print(page_url_list)

        for i in range(0, len(page_url_list)):
            gril_page_url = page_url_list[i]
            print(gril_page_url)
            yield Request(url=gril_page_url, callback=self.next3)
        pass

    def next3(self, response):
        pass
</code></pre>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480203708834.png" alt="Alt text" /></p>

<p>运行程序看看：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480203763527.png" alt="Alt text" /></p>

<hr />

<p><code>next3()</code> 这个回调函数的功能就是得到一个美女网页里面的所有的页面的网址。</p>

<p>有的美女的网页里面只有一个页面，有的美女的网页里面有多个页面：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1482656828793.png" alt="Alt text" /></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1482656883815.png" alt="Alt text" /></p>

<p>可以统一解决。</p>

<hr />

<p>测试：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480204459567.png" alt="Alt text" /></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480204419920.png" alt="Alt text" /></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480205200021.png" alt="Alt text" /></p>

<p>输出：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480205472599.png" alt="Alt text" /></p>

<hr />

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1482656683950.png" alt="Alt text" /></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480205432817.png" alt="Alt text" /></p>

<p>同样的回调函数</p>

<p>输出：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480205458465.png" alt="Alt text" /></p>

<hr />

<p>所以，我们可以这样写程序：</p>

<p>当得到的页码为-3，说明这个美女的网页是单页的；如果得到的页码数不等于-3，说明这个美女的网页是多也的。</p>

<p>测试程序：</p>

<p>对于多页面的美女网页网址</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480229614160.png" alt="Alt text" /></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480229673684.png" alt="Alt text" /></p>

<p>运行输出：</p>

<pre><code>D:\WorkSpace\python_ws\python-large-web-crawler\secondDemo&gt;scrapy crawl pic_169bb --nolog
10
http://www.169bb.com/xiyangmeinv/2016/0717/36463.html
http://www.169bb.com/xiyangmeinv/2016/0717/36463_5.html
http://www.169bb.com/xiyangmeinv/2016/0717/36463_10.html
http://www.169bb.com/xiyangmeinv/2016/0717/36463_9.html
http://www.169bb.com/xiyangmeinv/2016/0717/36463_6.html
http://www.169bb.com/xiyangmeinv/2016/0717/36463_8.html
http://www.169bb.com/xiyangmeinv/2016/0717/36463_7.html
http://www.169bb.com/xiyangmeinv/2016/0717/36463_3.html
http://www.169bb.com/xiyangmeinv/2016/0717/36463_4.html
http://www.169bb.com/xiyangmeinv/2016/0717/36463_2.html

D:\WorkSpace\python_ws\python-large-web-crawler\secondDemo&gt;
</code></pre>

<p>对于单页面的美女网页：</p>

<pre><code class="python">    def parse(self, response):
        ...
        yield Request(url='http://www.169bb.com/xiyangmeinv/2016/0103/2268.html', callback=self.demo)
</code></pre>

<p>回调函输一样。</p>

<p>运行输出：</p>

<pre><code class="python">D:\WorkSpace\python_ws\python-large-web-crawler\secondDemo&gt;scrapy crawl pic_169bb --nolog
-3
http://www.169bb.com/xiyangmeinv/2016/0103/2268.html

D:\WorkSpace\python_ws\python-large-web-crawler\secondDemo&gt;
</code></pre>

<p>成功。</p>

<p>所以现在的爬虫代码应该是这样的：</p>

<pre><code class="python"># -*- coding: utf-8 -*-
import scrapy
from scrapy.http import Request

class Pic169bbSpider(scrapy.Spider):
    name = "pic_169bb"
    allowed_domains = ["169bb.com"]
    start_urls = ['http://169bb.com/']

    def parse(self, response):
        title_list = response.xpath("/html/body/div[@class='header']/div[@class='hd_nav']/div[@class='w1000']//a/text()").extract()
        # print(title_list)
        urldata = response.xpath("/html/body/div[@class='header']/div[@class='hd_nav']/div[@class='w1000']//a/@href").extract()
        #print(urldata)
        xiyang_title = title_list[4] # 获取西洋美女标签的文本内容
        xiyang_urldata = urldata[4]  # 获取西洋美女首页网址
        # print(xiyang_title, xiyang_urldata)
        yield Request(url=xiyang_urldata, callback=self.next)
        # yield Request(url='http://www.169bb.com/xiyangmeinv/2016/0717/36463.html', callback=self.demo)
        # yield Request(url='http://www.169bb.com/xiyangmeinv/2016/0103/2268.html', callback=self.demo)

    def next(self, response):
        page_num_str = response.xpath("//span[@class='pageinfo']//strong/text()").extract()[0] # 得到西洋美女总页数
        # print(page_num_str)
        # print(response.url)
        for i in range(1, int(page_num_str)+1):
            page_url = response.url + 'list_4_'+ str(i) + '.html' # 得到西洋美女每一个页面的网址
            # print(page_url)
            yield Request(url=page_url, callback=self.next2)
        pass

    def next2(self, response):
        page_title_list = response.xpath("/html/body//div[@class='w1000 box03']/ul[@class='product01']//li/a/@alt").extract()
        # print(page_title_list)
        page_url_list = response.xpath("/html/body//div[@class='w1000 box03']/ul[@class='product01']//li/a/@href").extract()
        # print(page_url_list)

        for i in range(0, len(page_url_list)):
            gril_page_url = page_url_list[i] # 得到西洋美女页面里面每一个美女的网页网址
            print(gril_page_url)
            yield Request(url=gril_page_url, callback=self.next3)
        pass

    def next3(self, response):
        rela_pages_list = response.xpath("//div[@class='dede_pages']/ul//li/a/text()").extract()
        pages_num = len(rela_pages_list) - 3
        # print(pages_num)
        self.getPic(response)
        if pages_num == -3:
            # pages_num = 1
            return
        for i in range(2, pages_num+1):
            girl_page_url = response.url.replace('.html', '_') + str(i) + '.html'
            # print(girl_page_url)
            yield Request(url=girl_page_url, callback=self.next4)
        pass

    # def demo(self, response):
    # #     rela_pages_list = response.xpath("//div[@class='dede_pages']/ul//li/a/text()").extract()
    # #     pages_num = len(rela_pages_list)-3
    # #     print(pages_num)
    # #     pass
    #     rela_pages_list = response.xpath("//div[@class='dede_pages']/ul//li/a/text()").extract()
    #     pages_num = len(rela_pages_list) - 3
    #     # print(pages_num)
    #     self.getPic(response)
    #     if pages_num == -3:
    #         # pages_num = 1
    #         return
    #     for i in range(2, pages_num+1):
    #         girl_page_url = response.url.replace('.html', '_') + str(i) + '.html'
    #         # print(girl_page_url)
    #         yield Request(url=girl_page_url, callback=self.next4)
    #     pass

    def next4(self, response):
        self.getPic(response)
        pass

    def getPic(self, response):
        print(response.url)
        pass
</code></pre>

<hr />

<p>现在，我们需要在<code>getPic()</code> 函数中获取每一个美女网页的每一个页面里面的所有高清图片。</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480231492634.png" alt="Alt text" /></p>

<pre><code class="python">    def getPic(self, response):
        # print(response.url)
        item = SeconddemoItem()
        item['url'] = response.xpath("//div[@class='big-pic']/div[@class='big_img']//p/img/@src").extract()
        print(item['url'])
        pass
</code></pre>

<p>测试运行：</p>

<p>测试多页的美女网页：</p>

<p>先将<code>parse()</code>函数的最后一行改为：<code>yield Request(url='http://www.169bb.com/xiyangmeinv/2016/0717/36463.html', callback=self.demo)</code></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480230996305.png" alt="Alt text" /></p>

<p>测试单页的美女网页：</p>

<p>将<code>parse()</code>函数的最后一行改为：<code>yield Request(url='http://www.169bb.com/xiyangmeinv/2016/0103/2268.html', callback=self.demo)</code></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480237427282.png" alt="Alt text" /></p>

<p>成功。</p>

<hr />

<h3>下载高清图片</h3>

<p>OK，现在我们就已经得到了所有西洋美女的所有高清图片的下载地址，我们在<code>piplines.py</code> 文件中使用它们。</p>

<p>删除了<code>next4()</code>回调函数。在<code>demo()</code>回调函数里面调用的都是<code>getPic()</code>回调函数。</p>

<pre><code class="python">    def demo(self, response):
    #     rela_pages_list = response.xpath("//div[@class='dede_pages']/ul//li/a/text()").extract()
    #     pages_num = len(rela_pages_list)-3
    #     print(pages_num)
    #     pass
        rela_pages_list = response.xpath("//div[@class='dede_pages']/ul//li/a/text()").extract()
        pages_num = len(rela_pages_list) - 3
        # print(pages_num)
        self.getPic(response)
        if pages_num == -3:
            # pages_num = 1
            return
        for i in range(2, pages_num+1):
            girl_page_url = response.url.replace('.html', '_') + str(i) + '.html'
            # print(girl_page_url)
            yield Request(url=girl_page_url, callback=self.getPic)
        pass

    # error : yield 经过了一个中间函数，运行就有问题。我现在还不知道为什么
    # def next4(self, response):
    #     self.getPic(response)
    #     pass
</code></pre>

<p>并将<code>getPic()</code>函数里面的item写到生成器里面：</p>

<pre><code class="python">    def getPic(self, response):
        # print(response.url)
        item = SeconddemoItem()
        item['url'] = response.xpath("//div[@class='big-pic']/div[@class='big_img']//p/img/@src").extract()
        # print(item['url'])
        # pass
        yield item
</code></pre>

<p>测试：</p>

<p>测试单页的美女网页：</p>

<p>将<code>parse()</code>函数的最后一行改为：<code>yield Request(url='http://www.169bb.com/xiyangmeinv/2016/0103/2268.html', callback=self.getPic)</code></p>

<p>成功。</p>

<p>测试多页的美女网页：</p>

<p>先将<code>parse()</code>函数的最后一行改为：<code>yield Request(url='http://www.169bb.com/xiyangmeinv/2016/0717/36463.html', callback=self.demo)</code></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480230996305.png" alt="Alt text" /></p>

<p>也算算成功，因为有重复的文件名，所以自动替换，所以这里需要做修改。</p>

<p>我们可以使用美女图片网址里面的数字作为图片文件名的固定前缀来给图片命名，使用正则表达式获取网址的数字。</p>

<hr />

<p>在 <code>pipelines.py</code> 文件 中的 <code>process_item()</code> 函数中使用正则表达式得到图片下载网址的数字：</p>

<pre><code class="python">import re
import urllib.request

class SeconddemoPipeline(object):
    def process_item(self, item, spider):
        # print(len(item['url']))
        for i in range(0, len(item['url'])):
            this_url = item['url'][i]
            id = re.findall('http://724.169pp.net/169mm/(.*?).jpg', this_url)[0]
            id = id.replace('/', '_')
            print(id)
            # file = 'D:/WorkSpace/python_ws/python-large-web-crawler/xiyangmeinv/' + str(i) + '.jpg'
            # print('Downloading :' , file)
            # urllib.request.urlretrieve(this_url, filename=file)
            # print('Final Download :' , file)
        return item
</code></pre>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480241683726.png" alt="Alt text" /></p>

<p>要想使用 <code>pipelines.py</code> 文件 中的 <code>SeconddemoPipeline</code> 类，需要在 <code>settings.py</code> 文件里面设置 <code>ITEM_PIPELINES</code> 项：</p>

<pre><code class="python">ITEM_PIPELINES = {
   'secondDemo.pipelines.SeconddemoPipeline': 300,
}
</code></pre>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480242336499.png" alt="Alt text" /></p>

<blockquote><p>这里，我有一件事情不懂，关于正则表达式的： 网址里面的<code>.</code> 也是正则表达式中的工具字符，也是数据中中的内容，那么正则表达式是如何分辨它在这里是功能字符还是内容字符？</p></blockquote>

<p>在<code>pic_169bb.py</code> 文件的<code>demo()</code> 回调函数中，这样写才能获取到美女网页的第一页的图片地址：</p>

<pre><code class="python">        # error
        # self.getPic(response)
        # succes 为啥将下面的代码用self.getPic(response)的形式不能正常的获取到，而使用下面的代码却能获取到？
        item = SeconddemoItem()
        item['url'] = response.xpath("//div[@class='big-pic']/div[@class='big_img']//p/img/@src").extract()
        # print(item['url'])
        # pass
        yield item
</code></pre>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480241496168.png" alt="Alt text" /></p>

<p>运行程序试试：</p>

<p>测试单页的美女网页：</p>

<p>将<code>parse()</code>函数的最后一行改为：<code>yield Request(url='http://www.169bb.com/xiyangmeinv/2016/0103/2268.html', callback=self.getPic)</code></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480241922348.png" alt="Alt text" /></p>

<p>成功。</p>

<p>测试多页的美女网页：</p>

<p>先将<code>parse()</code>函数的最后一行改为：<code>yield Request(url='http://www.169bb.com/xiyangmeinv/2016/0717/36463.html', callback=self.demo)</code></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480241935773.png" alt="Alt text" /></p>

<p>都成功。</p>

<hr />

<p>先测试下载图片：</p>

<p>将 <code>pipelines.py</code> 文件 中的 <code>SeconddemoPipeline</code> 类的<code>process_item()</code> 函数里面，添加代码：</p>

<pre><code class="python">    def process_item(self, item, spider):
        # print(len(item['url']))
        for i in range(0, len(item['url'])):
            this_url = item['url'][i]
            id = re.findall('http://724.169pp.net/169mm/(.*?).jpg', this_url)[0]
            id = id.replace('/', '_')
            # print(id)
            file = 'D:/WorkSpace/python_ws/python-large-web-crawler/xiyangmeinv/' + id + '.jpg'
            print('Downloading :' , file)
            urllib.request.urlretrieve(this_url, filename=file)
            print('Final Download :' , file)
        return item
</code></pre>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480242553614.png" alt="Alt text" /></p>

<p>运行程序，没有毛病：（除了下载速度有点慢）</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480242613763.png" alt="Alt text" /></p>

<hr />

<h2>下载 169美女图片网 的所有西洋美女的图片</h2>

<p>在 <code>pic_169bb.py</code>文件里， 将<code>parse()</code>函数的最后一行改为：<code>yield Request(url=xiyang_urldata, callback=self.next)</code></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480242895996.png" alt="Alt text" /></p>

<p>将 <code>demo()</code> 函数里面的所有代码复制一份到 <code>next3()</code>函数里：</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480242867282.png" alt="Alt text" /></p>

<p>现在，运行程序：（最终的程序）</p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480243169351.png" alt="Alt text" /></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480243201173.png" alt="Alt text" /></p>

<p>成功！</p>

<hr />

<h2>防反爬技术</h2>

<p><strong>Step 4 . </strong> 不遵循 <code>robots.txt</code> 协议。</p>

<p>将 <code>settings.py</code> 文件里面的 <code>ROBOTSTXT_OBEY</code> 项设置为：<code>False</code></p>

<p><img src="/images/2016-12-26-python3-large-web-crawler-169bb-com-HD-beautiful-pictures/1480195250963.png" alt="Alt text" /></p>

<hr />

<p><strong>Step 6 . </strong> 模仿浏览器</p>

<blockquote><p>请先查看这篇博客：<a href="http://blog.csdn.net/github_35160620/article/details/52489709">http://blog.csdn.net/github_35160620/article/details/52489709</a> 里面是：<strong>六 . 设置 用户代理（user_agent）</strong>。</p></blockquote>

<p>将 <code>settings.py</code> 文件里面的 <code>USER_AGENT</code> 项设置为：浏览器的用户代理信息。</p>

<pre><code class="python">USER_AGENT = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.22 Safari/537.36 SE 2.X MetaSr 1.0'
</code></pre>

<hr />

<p><strong>Step 7 . </strong>  禁止缓存</p>

<p>将 <code>settings.py</code> 文件里面的 <code>COOKIES_ENABLED</code> 项设置为：<code>False</code>。</p>

<pre><code class="python">COOKIES_ENABLED = False
</code></pre>

<hr />

<h2>搞定</h2>

<hr />

<p>需要升级的地方:（2016-11-27 19:34:34）</p>

<ol>
<li>在易错的代码段加上异常检测程序</li>
<li>在下载图片的代码加上：超时异常检测程序</li>
<li>记录成功下载的、超时失败下载的、链接失败下载的 信息</li>
<li>添加断点续下功能。</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python3 大型网络爬虫实战 — 给 scrapy 爬虫项目设置为防反爬]]></title>
    <link href="http://aobojaing.github.io/blog/2016/12/06/python3-large-web-crawler-scrapy-project-Anti-reptile-settings/"/>
    <updated>2016-12-06T00:04:35+08:00</updated>
    <id>http://aobojaing.github.io/blog/2016/12/06/python3-large-web-crawler-scrapy-project-Anti-reptile-settings</id>
    <content type="html"><![CDATA[<hr />

<h2>开发环境</h2>

<ul>
<li>Python第三方库：lxml、Twisted、pywin32、scrapy</li>
<li>Python 版本：python-3.5.0-amd64</li>
<li>PyCharm软件版本：pycharm-professional-2016.1.4</li>
<li>电脑系统：Windows 10 64位</li>
</ul>


<p>如果你还没有搭建好开发环境，请到<a href="http://www.aobosir.com/blog/2016/11/26/python3-large-web-crawler-001-Build-development-environment/">这篇博客</a>。</p>

<hr />

<p>所有的设置都是在scrapy爬虫项目中的<code>settings.py</code> 文件中进行设置。</p>

<p><strong>Step 1 . </strong> 设置爬虫不遵循 <code>robots.txt</code>协议</p>

<pre><code># Obey robots.txt rules
ROBOTSTXT_OBEY = False
</code></pre>

<p><img src="/images/2016-12-6-python3-large-web-crawler-scrapy-project-Anti-reptile-settings/1480952600971.png" alt="Alt text" /></p>

<blockquote><p>想要了解什么是<code>robots.txt</code>协议，请访问这篇博客：<a href="http://blog.csdn.net/github_35160620/article/details/52586126">解析 robots.txt 文件</a>。</p></blockquote>

<p><strong>Step 2 . </strong> 设置取消<strong>Cookies</strong></p>

<pre><code class="python"># Disable cookies (enabled by default)
COOKIES_ENABLED = False
</code></pre>

<p><img src="/images/2016-12-6-python3-large-web-crawler-scrapy-project-Anti-reptile-settings/1480952959564.png" alt="Alt text" /></p>

<blockquote><p><strong>Cookies</strong>：</p>

<p> 简单的说，Cookie就是服务器暂存放在你计算机上的一笔资料，好让服务器用来辨认你的计算机。当你在浏览网站的时候，Web服务器会先送一小小资料放在你的计算机上，Cookie 会帮你在网站上所打的文字或是一些选择，都记录下来。当下次你再光临同一个网站，Web服务器会先看看有没有它上次留下的Cookie资料，有的话，就会依据Cookie里的内容来判断使用者，送出特定的网页内容给你。</p></blockquote>

<p><strong>Step 3 . </strong> 设置用户代理值（<code>USER_AGENT</code>）</p>

<pre><code class="python"># Crawl responsibly by identifying yourself (and your website) on the user-agent
USER_AGENT = 'Mozilla/xxx (Windows xxx; Winxx; xxx) AppleWebKit/xxx (KHTML, like Gecko) Chrome/xxxx Safari/xxx'
</code></pre>

<p><img src="/images/2016-12-6-python3-large-web-crawler-scrapy-project-Anti-reptile-settings/1480953048379.png" alt="Alt text" /></p>

<p>这个 用户代理可以在浏览器里面找到：</p>

<p>随便浏览一个网页，按<strong>F12</strong> -> <strong>Network</strong> -> <strong>F5</strong>，随便点击一项，你都能看到有 <strong>User-agent</strong> 这一项，将这里面的内容拷贝就可以。</p>

<p><img src="/images/2016-12-6-python3-large-web-crawler-scrapy-project-Anti-reptile-settings/1480953359818.png" alt="Alt text" /></p>

<p><strong>Step 4 . </strong> 设置IP</p>

<p>对于这一步，如果你没有做什么违法的事情，可以不用设置。仅仅上面的三个步骤，就可以将那些具有反爬虫机制的网站可以正常爬取了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python3 大型网络爬虫实战 002 --- scrapy 爬虫项目的创建及爬虫的创建 --- 实例：爬取百度标题和CSDN博客]]></title>
    <link href="http://aobojaing.github.io/blog/2016/11/26/python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn/"/>
    <updated>2016-11-26T18:22:27+08:00</updated>
    <id>http://aobojaing.github.io/blog/2016/11/26/python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn</id>
    <content type="html"><![CDATA[<hr />

<h2>开发环境</h2>

<ul>
<li>Python第三方库：lxml、Twisted、pywin32、scrapy</li>
<li>Python 版本：python-3.5.0-amd64</li>
<li>PyCharm软件版本：pycharm-professional-2016.1.4</li>
<li>电脑系统：Windows 10 64位</li>
</ul>


<p>如果你还没有搭建好开发环境，请到<a href="http://www.aobosir.com/blog/2016/11/26/python3-large-web-crawler-001-Build-development-environment/">这篇博客</a>。</p>

<hr />

<h1>1 知识点：scrapy 爬虫项目的创建及爬虫的创建</h1>

<h2>1.1 scrapy 爬虫项目的创建</h2>

<p>接下来我们为大家创建一个Scrapy爬虫项目，并在爬虫项目下创建一个Scrapy爬虫文件。</p>

<pre><code>scrapy startproject &lt;projectname&gt; 
</code></pre>

<h2>1.2 scrapy 爬虫文件的创建</h2>

<pre><code>cd demo
scrapy genspider -t basic &lt;filename&gt; &lt;domain&gt;
</code></pre>

<blockquote><p>更多 <strong>Scrapy</strong> 命令的介绍请到<a href="http://www.aobosir.com/blog/2016/11/26/python-Scrapy-command/">这篇博客</a>查看。</p></blockquote>

<hr />

<h1>2 实例：爬取百度标题和CSDN博客</h1>

<p>我们创建一个爬虫项目，在里面创建一个爬虫文件来爬取百度，并再创建一个爬虫文件爬取CSDN博客文章。</p>

<hr />

<p>先创建一个Scrapy爬虫项目：</p>

<pre><code>scrapy startproject firstDemo
</code></pre>

<p>输出：</p>

<pre><code>D:\WorkSpace\python_ws\python-large-web-crawler&gt;scrapy startproject firstdemo
New Scrapy project 'firstdemo', using template directory 'c:\\users\\aobo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\scrapy\\templates\\project', created in:
    D:\WorkSpace\python_ws\python-large-web-crawler\firstdemo

You can start your first spider with:
    cd firstdemo
    scrapy genspider example example.com

D:\WorkSpace\python_ws\python-large-web-crawler&gt;
</code></pre>

<h2>2-1.1 使用Scrapy爬虫 爬取百度标题</h2>

<p>创建一个爬虫文件来爬取百度</p>

<pre><code>cd firstDemo
scrapy genspider -t basic baidu baidu.com
</code></pre>

<p>输出：</p>

<pre><code>D:\WorkSpace\python_ws\python-large-web-crawler&gt;cd firstdemo

D:\WorkSpace\python_ws\python-large-web-crawler\firstdemo&gt;scrapy genspider -t basic baidu baidu.com
Created spider 'baidu' using template 'basic' in module:
  firstdemo.spiders.baidu

D:\WorkSpace\python_ws\python-large-web-crawler\firstdemo&gt;
</code></pre>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn/1480084611306.png" alt="Alt text" /></p>

<p>打开 <strong>PyCharm</strong> 软件，用 <strong>PyCharm</strong> 软件打开刚刚创建的 <code>firstdemo</code> 爬虫项目。</p>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn//1480084757859.png" alt="Alt text" /></p>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn//1480084783287.png" alt="Alt text" /></p>

<p>打开这 <code>baidu.py</code> 爬虫文件，你会看到自动生成的代码：</p>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn//1480084952255.png" alt="Alt text" /></p>

<h2>2-1.2 观察 <a href="https://www.baidu.com/">https://www.baidu.com/</a> 网页源代码</h2>

<p>（源代码太多，列出重点的。）</p>

<pre><code class="http">&lt;html xmlns="http://www.w3.org/1999/xhtml" class="cye-enabled cye-nm sui-componentWrap"&gt;
    &lt;head&gt;
        &lt;title&gt;百度一下，你就知道 &lt;/title&gt;
    &lt;/head&gt;
&lt;/html&gt;
</code></pre>

<p>源代码中的标题通过标签逐步定位： <code>/html/head/title</code></p>

<h2>2-1.3 写代码</h2>

<p>我们现在要提取出 <a href="https://www.baidu.com/">https://www.baidu.com/</a> 网页 的标题：<strong>百度一下，你就知道</strong>。</p>

<p>提取信息，一般使用 <code>xpath</code> 或者 <strong>正则表达式</strong> 来提取。</p>

<p>这里我们使用 <code>xpath</code> 来提取，<code>xpath</code> 的知识点，请到<a href="http://www.aobosir.com/blog/2016/11/26/python-xpath/">这篇博客</a>中查看。</p>

<hr />

<p>下面的编写代码的步骤：</p>

<p><strong>Step 1 . </strong> 设置我们的爬虫不遵循 <code>robots.txt</code> 规定。（什么是<code>robots.txt</code>规定，请到<a href="http://blog.csdn.net/github_35160620/article/details/52586126">这个博客</a>查看。）</p>

<p>打开 <code>settings.py</code> 文件，将里面的<code>ROBOTSTXT_OBEY</code> 设为：<code>False</code></p>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn//1480086841701.png" alt="Alt text" /></p>

<p><strong>Step 2 . </strong>  打开 <code>items.py</code> 文件，在里面<code>FirstdemoItem()</code>函数里添加一项：</p>

<pre><code>    title = scrapy.Field()
</code></pre>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn//1480087390745.png" alt="Alt text" /></p>

<p><strong>Step 3 . </strong>  在 <code>baidu.py</code> 文件里面，使用<code>xpath 表达式</code> 提取百度网页的标题。</p>

<p>先从核心目录（<code>firstdemo</code>）定位到<code>items.py</code> 文件里面的<code>FirstdemoItem</code>函数。</p>

<p>然后使用<code>xpath 表达式</code> 提取百度网页的标题。</p>

<p>最后，返回。</p>

<pre><code class="python"># -*- coding: utf-8 -*-
import scrapy
from firstdemo.items import FirstdemoItem

class BaiduSpider(scrapy.Spider):
    name = "baidu"
    allowed_domains = ["baidu.com"]
    start_urls = ['http://baidu.com/']

    def parse(self, response):
        item = FirstdemoItem()
        item['title'] = response.xpath('/html/head/title/text()').extract()
        yield item
</code></pre>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn//1480088827529.png" alt="Alt text" /></p>

<p><strong>Step 4 . </strong>
在 <code>pipelines.py</code> 文件里面的<code>FirstdemoPipeline()</code>函数，添加打印信息的代码：</p>

<pre><code class="python">        print(item['title'])
</code></pre>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn//1480087767661.png" alt="Alt text" /></p>

<p>但是，现在运行程序，是不能输出任何信息的，还需要做<strong>Step 5</strong>。</p>

<p><strong>Step 5 . </strong> 开启<code>piplines</code>（默认<code>piplines</code>是关闭的。）
在 <code>settings.py</code> 文件，将里面的<code>ITEM_PIPELINES</code> 项的注释去掉。并从核心目录开始定位，定位到<code>pipelines.py</code> 文件里面的<code>FirstdemoPipeline()</code>函数 ，就应该是：<code>firstdemo.pipelines.FirstdemoPipeline</code>：</p>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn//1480088423683.png" alt="Alt text" /></p>

<h2>2-1.4 运行</h2>

<p>在 <strong>DOS窗口</strong> 中，先将路劲切换到当前爬虫项目<code>firstdemo</code>路径下，然后在执行爬虫文件 <code>baidy</code></p>

<pre><code>D:
cd D:\WorkSpace\python_ws\python-large-web-crawler\firstdemo
scrapy crawl baidu --nolog
</code></pre>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn//1480089009819.png" alt="Alt text" /></p>

<hr />

<h2>2-2.1 使用Scrapy爬虫CSDN的博客文章</h2>

<p>创建一个爬虫文件爬取CSDN博客文章。</p>

<pre><code>scrapy genspider -t basic csdn blog.csdn.net
</code></pre>

<p>输出:</p>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn//1480089722224.png" alt="Alt text" /></p>

<h2>2-2.2 观察 <a href="http://blog.csdn.net/">http://blog.csdn.net/</a> 网页源代码</h2>

<p>（网页源代码太多，这里就不贴出了。）</p>

<p>提取信息，一般使用 <code>xpath</code> 或者 <strong>正则表达式</strong> 来提取。</p>

<h2>2-2.3 写代码</h2>

<p><strong>Step 1 . </strong>  在<code>items.py</code> 文件中的<code>FirstdemoItem()</code>函数中添加新的项。其他的文件会使用这几个对象：</p>

<pre><code class="python">    detail = scrapy.Field()
    link = scrapy.Field()
</code></pre>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn//1480089896612.png" alt="Alt text" /></p>

<p><strong>Step 2 . </strong>  在 <code>csdn.py</code> 文件里面，使用<code>xpath 表达式</code> 提取csdn博客网页的博文标题、介绍、链接地址。</p>

<pre><code class="python"># -*- coding: utf-8 -*-
import scrapy
from firstdemo.items import FirstdemoItem


class CsdnSpider(scrapy.Spider):
    name = "csdn"
    allowed_domains = ["blog.csdn.net"]
    start_urls = ['http://blog.csdn.net/']

    def parse(self, response):
        item = FirstdemoItem()
        item['title'] = response.xpath("//h3[@class='tracking-ad']/a/text()").extract()
        item['detail'] = response.xpath("//div[@class='blog_list_c']/text()").extract()
        item['link'] = response.xpath("//h3[@class='tracking-ad']/a/@href").extract()
        yield item
</code></pre>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn//1480090250536.png" alt="Alt text" /></p>

<p><strong>Step 3 . </strong> 在 <code>piplines.py</code> 文件中，添加下面的代码，输出显示爬取到的信息。</p>

<pre><code class="python">        for i in range(0, len(item['title'])):
            print('第' + str(i+1) + '篇文章：')
            print(item['title'][i])
            print(item['detail'][i])
            print(item['link'][i])
            print('---------')
</code></pre>

<p><img src="/images/2016-11-26-python3-large-web-crawler-002-scrapy-crawler-project-create-baidu-csdn//1480090494545.png" alt="Alt text" /></p>

<h2>2-1.4 运行</h2>

<pre><code>scrapy crawl csdn --nolog
</code></pre>

<blockquote><p>执行输出的信息太少，说明程序有问题。</p>

<pre><code>scrapy crawl csdn
</code></pre>

<p>如果你在执行的时候，找到错误提示信息：</p>

<pre><code>UnicodeEncodeError: 'gbk' codec can't encode character '\xa0' in position 10: illegal multibyte sequence
</code></pre>

<p>这个问题经常会遇到，是一个常见的问题，解决办法<a href="http://www.aobosir.com/blog/2016/11/26/python3-UnicodeEncodeError-gbk-codec-can%27t-encode-character-xa0/">在这里</a>可以找到。</p>

<pre><code class="python">         print(item['detail'][i].replace(u'\xa0 ', u' '))
</code></pre></blockquote>

<p>输出：</p>

<pre><code>第1篇文章：
微信小程序：小程序，新场景
前言：我们频繁进入的地方，是场景。手机，是场景；浏览器，是场景；其实，微信，也是场景……微信要做的是占据更多用户时间、占
据更多应用场景、占据更多服务入口，这是商业本质想去垄断要做的事情。对于大家来讲，...
http://blog.csdn.net/liujia216/article/details/53350247
---------
第2篇文章：
Android四大组件——BroadcastReceiver普通广播、有序广播、拦截广播、本地广播、Sticky广播、系统广播
BroadcastReceiver普通广播、有序广播、拦截广播、本地广播、Sticky广播、系统广播
  本篇文章包括以下内容：


  前言
  BroadcastReceiver的简介
 ...
http://blog.csdn.net/qq_30379689/article/details/53341313
---------
第3篇文章：
Gif格式简要介绍
Gif格式的介绍

为什么有的Gif图不能够循环播放及处理办法
http://blog.csdn.net/shiroh_ms08/article/details/53347873
---------
第4篇文章：
win10 uwp 打包第三方字体到应用
有时候我们会把一些特殊字体打包到软件，因为如果找不到我们的字体会变为默认，现在很多字体图标我们用得好，有时候我们的应用会
用很漂亮的字体，需要我们自己打包，因为用户一般是没有字体。UWP使用第三方字体首...
http://blog.csdn.net/lindexi_gd/article/details/52716655
---------
第5篇文章：
话说智能指针发展之路
动态创建内存的管理太麻烦，于是乎，这个世界变成11派人：
一派人勤勤恳恳按照教科书的说法做，时刻小心翼翼，苦逼连连；
一派人忘记教科书的教导，随便乱来，搞得代码处处bug，后期维护骂声连连；
最...
http://blog.csdn.net/jacketinsysu/article/details/53343534
---------
第6篇文章：
安卓自定义控件（二）BitmapShader、ShapeDrawable、Shape
第一篇博客中，我已经对常用的一些方法做了汇总，这篇文章主要介绍BitmapShader位图渲染、ComposeShader组合渲染，然后看看Xferm
ode如何实际应用。不过本文还是只重写onDraw...
http://blog.csdn.net/chen413203144/article/details/53343209
---------
第7篇文章：
JSTL 标签大全详解
1、什么是JSTL？    JSTL是apache对EL表达式的扩展（也就是说JSTL依赖EL），JSTL是标签语言！JSTL标签使用以来非常方便，它与JSP
动作标签一样，只不过它不是JSP内...
http://blog.csdn.net/qq_25827845/article/details/53311722
---------
第8篇文章：
Android调试大法 自定义IDE默认签名文件
你是否为调试第三方SDK时debug签名和release签名发生冲突而烦恼？你是否在debug时第三方功能测试通过，而release时无法使用？你
是否在为对接微信、支付宝、地图因签名导致的问题而烦恼？...
http://blog.csdn.net/yanzhenjie1003/article/details/53334071
---------
第9篇文章：
Android图表库MPAndroidChart(十二)——来点不一样的，正负堆叠条形图
Android图表库MPAndroidChart(十二)——来点不一样的，正负堆叠条形图
  接上篇，今天要说的，和上篇的类似，只是方向是有相反的两面，我们先看下效果  实际上这样就导致了我们的代码是...
http://blog.csdn.net/qq_26787115/article/details/53333270
---------
第10篇文章：
一步步手动实现热修复(二)-类的加载机制简要介绍
一个类在被加载到内存之前要经过加载、验证、准备等过程。经过这些过程之后，虚拟机才会从方法区将代表类的运行时数据结构转换为
内存中的Class。

我们这节内容的重点在于一个类是如何被加载的，所以我们从类...
http://blog.csdn.net/sahadev_/article/details/53334911
---------
第11篇文章：
仿射变换详解 warpAffine
今天遇到一个问题是关于仿射变换的，但是由于没有将仿射变换的具体原理型明白，看别人的代码看的很费解，最后终于在师兄的帮助下
将原理弄明白了，我觉得最重要的是理解仿射变换可以看成是几种简单变换的复合实现，
...
http://blog.csdn.net/q123456789098/article/details/53330484
---------
第12篇文章：
React Native嵌入Android原生应用中
开发环境准备首先你要搭建好React Native for Android开发环境， 没有搭建好的可以参考：React Native for Android Windows环境
搭建  用Android...
http://blog.csdn.net/u011965040/article/details/53331859
---------
第13篇文章：
TCP三次握手四次挥手详解
TCP三次握手四次挥手详解
http://blog.csdn.net/u010913001/article/details/53331863
---------
第14篇文章：
腾讯Android面经
秋招收官最后一战。
腾讯一面（电话）：
自我介绍
项目，平时怎么学习？
设计模式
（1）知道哪些设计模式？设计模式在Android、Java中是怎么应用的，每个都说一下？
（2）InputStre...
http://blog.csdn.net/kesarchen/article/details/53332157
---------
第15篇文章：
轻松实现部分背景半透明的呈现效果
实现一个简单的呈现/解散动画效果，当呈现时，呈现的主要内容和背景要明显区分，背景呈现一个半透明遮罩效果，透过背景可以看到
下层 View Controller 的内容
http://blog.csdn.net/kmyhy/article/details/53322669
---------
第16篇文章：
APP自动化框架LazyAndroid使用手册（4）--测试模板工程详解
概述前面的3篇博文分别对lazyAndroid的框架简介、元素抓取和核心API进行了说明，本文将基于框架给出的测试模板工程，详细阐述下
使用该框架进行安卓UI自动化测试的步骤。
http://blog.csdn.net/kaka1121/article/details/53325265
---------
第17篇文章：
Android使用getIdentifier()方法根据资源名来获取资源id
有时候我们想动态的根据一个资源名获得到对应的资源id，就可以使用getResources().getIdentifier()方法来获取该id。然后再使用该
id进行相关的操作。
1、Demo示例
  下...
http://blog.csdn.net/ouyang_peng/article/details/53328000
---------
第18篇文章：
Android基于RecyclerView实现高亮搜索列表
这篇应该是RecycleView的第四篇了，RecycleView真是新生代的宠儿能做这么多的事情。转载请注明作者AndroidMsky及原文链接
http://blog.csdn.net/and...
http://blog.csdn.net/androidmsky/article/details/53306657
---------
第19篇文章：
使用Git Hooks实现开发部署任务自动化
提供：ZStack云计算 前言版本控制，这是现代软件开发的核心需求之一。有了它，软件项目可以安全的跟踪代码变更并执行回溯、完整
性检查、协同开发等多种操作。在各种版本控制软件中，git是近年来最流行的软...
http://blog.csdn.net/zstack_org/article/details/53331077
---------
第20篇文章：
Andromeda OS 来了，Android 再见？
相信有部分同学已经有耳闻了，前几天炒的很火一个消息，就是 Google 要推出一种全新的操作系统，取名 Andromeda，这款新型的操作
系统融合了 Android 和 Chrome OS，据称已经有...
http://blog.csdn.net/googdev/article/details/53331364
---------
</code></pre>

<hr />

<p>我用英语跟小贩交谈，突然画面一下就全暗，我回台上，终于轮我上场。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python --- xpath 表达式 --- Ongoing]]></title>
    <link href="http://aobojaing.github.io/blog/2016/11/26/python-xpath/"/>
    <updated>2016-11-26T18:00:15+08:00</updated>
    <id>http://aobojaing.github.io/blog/2016/11/26/python-xpath</id>
    <content type="html"><![CDATA[<hr />

<h2>什么是 <strong>XPath</strong>？</h2>

<p>参考网站：<a href="http://www.w3school.com.cn/xpath/xpath_intro.asp">http://www.w3school.com.cn/xpath/xpath_intro.asp</a></p>

<p>XPath 是一门在 XML 文档中查找信息的语言。XPath 用于在 XML 文档中通过元素和属性进行导航。</p>

<h2>xpath 表达式 语法讲解</h2>

<p>参考网站：<a href="http://www.w3school.com.cn/xpath/xpath_syntax.asp">http://www.w3school.com.cn/xpath/xpath_syntax.asp</a></p>

<p>例如，现在有这些信息：</p>

<pre><code>&lt;head&gt;
&lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt;
&lt;title&gt;CSDN.NET - 全球最大中文IT社区，为IT专业技术人员提供最全面的信息传播和服务平台&lt;/title&gt;
&lt;link href="http://c.csdnimg.cn/www/css/csdn_common.css" rel="stylesheet" type="text/css"&gt;
&lt;link href="css/content.css" rel="stylesheet" type="text/css"&gt;
&lt;link href="http://c.csdnimg.cn/public/favicon.ico" rel="SHORTCUT ICON"&gt;
&lt;/head&gt;
</code></pre>

<table>
<thead>
<tr>
<th style="text-align:left;"> 表达式</th>
<th style="text-align:right;">     意思</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">/ </td>
<td style="text-align:right;">寻找指定标签</td>
</tr>
<tr>
<td style="text-align:left;">//</td>
<td style="text-align:right;"> 寻找所有标签</td>
</tr>
<tr>
<td style="text-align:left;">@</td>
<td style="text-align:right;"> 提取某个标签的属性的内容 </td>
</tr>
</tbody>
</table>


<hr />

<p><strong>例子：</strong></p>

<p>不管怎么样，记住一点：<strong>只有唯一的东西才能定位</strong>。</p>

<pre><code class="python">/title.text()
</code></pre>

<p>表示：</p>

<pre><code>CSDN.NET - 全球最大中文IT社区，为IT专业技术人员提供最全面的信息传播和服务平台
</code></pre>

<hr />

<p>参考网站：<a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/selectors.html">http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/selectors.html</a></p>

<hr />
]]></content>
  </entry>
  
</feed>
