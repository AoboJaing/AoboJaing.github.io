<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Python3 大型网络爬虫实战 | AoboSir 博客]]></title>
  <link href="http://aobojaing.github.io/blog/categories/python3-da-xing-wang-luo-pa-chong-shi-zhan/atom.xml" rel="self"/>
  <link href="http://aobojaing.github.io/"/>
  <updated>2016-11-26T07:08:23+08:00</updated>
  <id>http://aobojaing.github.io/</id>
  <author>
    <name><![CDATA[Aobo Jaing]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[解决问题： Pywin32 安装后出现 Import Win32api ImportError DLL Load Failed]]></title>
    <link href="http://aobojaing.github.io/blog/2016/11/26/solve-pywin32-import-win32api-ImportError-DLL-load-failed/"/>
    <updated>2016-11-26T07:04:25+08:00</updated>
    <id>http://aobojaing.github.io/blog/2016/11/26/solve-pywin32-import-win32api-ImportError-DLL-load-failed</id>
    <content type="html"><![CDATA[<hr />

<p>执行 <code>scrapy bench</code> 命令时 出现错误。（之前安装了pywin32库）</p>

<pre><code>Traceback (most recent call last):
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\twisted\internet\defer.py", line 1260, in _inlineCallbacks
    result = g.send(result)
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\scrapy\crawler.py", line 72, in crawl
    self.engine = self._create_engine()
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\scrapy\crawler.py", line 97, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\scrapy\core\engine.py", line 68, in __init__
    self.downloader = downloader_cls(crawler)
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "&lt;frozen importlib._bootstrap&gt;", line 986, in _gcd_import
  File "&lt;frozen importlib._bootstrap&gt;", line 969, in _find_and_load
  File "&lt;frozen importlib._bootstrap&gt;", line 958, in _find_and_load_unlocked
  File "&lt;frozen importlib._bootstrap&gt;", line 673, in _load_unlocked
  File "&lt;frozen importlib._bootstrap_external&gt;", line 662, in exec_module
  File "&lt;frozen importlib._bootstrap&gt;", line 222, in _call_with_frames_removed
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\scrapy\downloadermiddlewares\retry.py", line 23, in &lt;module&gt;
    from scrapy.xlib.tx import ResponseFailed
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\scrapy\xlib\tx\__init__.py", line 3, in &lt;module&gt;
    from twisted.web import client
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\twisted\web\client.py", line 42, in &lt;module&gt;
    from twisted.internet.endpoints import TCP4ClientEndpoint, SSL4ClientEndpoint
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\twisted\internet\endpoints.py", line 36, in &lt;module&gt;
    from twisted.internet.stdio import StandardIO, PipeAddress
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\twisted\internet\stdio.py", line 30, in &lt;module&gt;
    from twisted.internet import _win32stdio
  File "c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages\twisted\internet\_win32stdio.py", line 9, in &lt;module&gt;
    import win32api
ImportError: DLL load failed: 找不到指定的模块。
</code></pre>

<h2>解决办法：</h2>

<p>参考网站：</p>

<p><a href="http://blog.csdn.net/mtt_sky/article/details/50445938">http://blog.csdn.net/mtt_sky/article/details/50445938</a>
<a href="http://blog.sina.com.cn/s/blog_5a81b7990101l225.html">http://blog.sina.com.cn/s/blog_5a81b7990101l225.html</a></p>

<p><code>C:\Users\AOBO\AppData\Local\Programs\Python\Python35\Lib\site-packages\pywin32_system32</code></p>

<p>里面的所有的文件复制到：<code>C:\Windows\System32</code></p>

<p>现在，问题解决。无需重新打开DOS窗口，直接执行：<code>scrapy bench</code>。</p>

<p>输出正常：</p>

<pre><code>D:\BaiduYunDownload\first&gt;scrapy bench
2016-11-23 13:56:45 [scrapy] INFO: Scrapy 1.2.1 started (bot: first)
2016-11-23 13:56:45 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'first.spiders', 'CLOSESPIDER_TIMEOUT': 10, 'LOGSTATS_INTERVAL': 1, 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'first', 'SPIDER_MODULES': ['first.spiders']}
2016-11-23 13:56:47 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.closespider.CloseSpider',
 'scrapy.extensions.logstats.LogStats']
2016-11-23 13:56:48 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-23 13:56:48 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-23 13:56:48 [scrapy] INFO: Enabled item pipelines:
['first.pipelines.FirstPipeline']
2016-11-23 13:56:48 [scrapy] INFO: Spider opened
2016-11-23 13:56:48 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-23 13:56:49 [scrapy] INFO: Crawled 69 pages (at 4140 pages/min), scraped 0 items (at 0 items/min)
2016-11-23 13:56:50 [scrapy] INFO: Crawled 141 pages (at 4320 pages/min), scraped 0 items (at 0 items/min)
2016-11-23 13:56:51 [scrapy] INFO: Crawled 205 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
2016-11-23 13:56:52 [scrapy] INFO: Crawled 269 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
2016-11-23 13:56:53 [scrapy] INFO: Crawled 325 pages (at 3360 pages/min), scraped 0 items (at 0 items/min)
2016-11-23 13:56:54 [scrapy] INFO: Crawled 373 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)
2016-11-23 13:56:55 [scrapy] INFO: Crawled 429 pages (at 3360 pages/min), scraped 0 items (at 0 items/min)
2016-11-23 13:56:56 [scrapy] INFO: Crawled 477 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)
2016-11-23 13:56:57 [scrapy] INFO: Crawled 533 pages (at 3360 pages/min), scraped 0 items (at 0 items/min)
2016-11-23 13:56:58 [scrapy] INFO: Crawled 581 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)
2016-11-23 13:56:58 [scrapy] INFO: Closing spider (closespider_timeout)
2016-11-23 13:56:59 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 265444,
 'downloader/request_count': 597,
 'downloader/request_method_count/GET': 597,
 'downloader/response_bytes': 1833261,
 'downloader/response_count': 597,
 'downloader/response_status_count/200': 597,
 'finish_reason': 'closespider_timeout',
 'finish_time': datetime.datetime(2016, 11, 23, 5, 56, 59, 266168),
 'log_count/INFO': 17,
 'request_depth_max': 20,
 'response_received_count': 597,
 'scheduler/dequeued': 597,
 'scheduler/dequeued/memory': 597,
 'scheduler/enqueued': 11938,
 'scheduler/enqueued/memory': 11938,
 'start_time': datetime.datetime(2016, 11, 23, 5, 56, 48, 450531)}
2016-11-23 13:56:59 [scrapy] INFO: Spider closed (closespider_timeout)

D:\BaiduYunDownload\first&gt;
</code></pre>

<hr />
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python --- Scrapy 命令]]></title>
    <link href="http://aobojaing.github.io/blog/2016/11/26/python-Scrapy-command/"/>
    <updated>2016-11-26T06:39:53+08:00</updated>
    <id>http://aobojaing.github.io/blog/2016/11/26/python-Scrapy-command</id>
    <content type="html"><![CDATA[<hr />

<p>Scrapy 命令 分为两种：<strong>全局命令</strong> 和 <strong>项目命令</strong>。</p>

<p>全局命令：在哪里都能使用。</p>

<p>项目命令：必须在爬虫项目里面才能使用。</p>

<h2>全局命令</h2>

<pre><code>C:\Users\AOBO&gt;scrapy -h
Scrapy 1.2.1 - no active project

Usage:
  scrapy &lt;command&gt; [options] [args]

Available commands:
  bench         Run quick benchmark test
  commands
  fetch         Fetch a URL using the Scrapy downloader
  genspider     Generate new spider using pre-defined templates
  runspider     Run a self-contained spider (without creating a project)
  settings      Get settings values
  shell         Interactive scraping console
  startproject  Create new project
  version       Print Scrapy version
  view          Open URL in browser, as seen by Scrapy

  [ more ]      More commands available when run from project directory

Use "scrapy &lt;command&gt; -h" to see more info about a command
</code></pre>

<ul>
<li><strong>startproject</strong>：创建一个爬虫项目：<code>scrapy startproject demo</code>（<code>demo</code> 创建的爬虫项目的名字）</li>
<li><strong>runspider</strong> 运用单独一个爬虫文件：<code>scrapy runspider abc.py</code></li>
<li><strong>veiw</strong> 下载一个网页的源代码，并在默认的文本编辑器中打开这个源代码：<code>scrapy view http://www.aobossir.com/</code></li>
<li><strong>shell</strong> 进入交互终端，用于爬虫的调试（如果你不调试，那么就不常用）：<code>scrapy shell http://www.baidu.com --nolog</code>（<code>--nolog</code> 不显示日志信息）</li>
<li><strong>version</strong> 查看版本：（<code>scrapy version</code>）</li>
<li><strong>bench</strong> 测试本地硬件性能（工作原理：）：<code>scrapy bench</code> （如果遇到问题：解决问题:  <code>import win32api ImportError: DLL load failed</code>，到这里查看解决办法。）</li>
</ul>


<hr />

<h2>项目命令</h2>

<p>（进入项目路径，才能看到项目命令）</p>

<pre><code>D:\BaiduYunDownload\first&gt;scrapy -h
Scrapy 1.2.1 - project: first

Usage:
  scrapy &lt;command&gt; [options] [args]

Available commands:
  bench         Run quick benchmark test
  check         Check spider contracts
  commands
  crawl         Run a spider
  edit          Edit spider
  fetch         Fetch a URL using the Scrapy downloader
  genspider     Generate new spider using pre-defined templates
  list          List available spiders
  parse         Parse URL (using its spider) and print the results
  runspider     Run a self-contained spider (without creating a project)
  settings      Get settings values
  shell         Interactive scraping console
  startproject  Create new project
  version       Print Scrapy version
  view          Open URL in browser, as seen by Scrapy

Use "scrapy &lt;command&gt; -h" to see more info about a command

D:\BaiduYunDownload\first&gt;
</code></pre>

<ul>
<li><strong>genspider</strong> 创建一个爬虫文件，我们在爬虫项目里面才能创建爬虫文件（这个命令用的非常多）（<strong>startproject</strong>：创建一个爬虫项目）。创建爬虫文件是按照以下模板来创建的，使用<code>scrapy genspider -l</code> 命令查看有哪些模板。</li>
</ul>


<pre><code>D:\BaiduYunDownload\first&gt;scrapy genspider -l
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed

D:\BaiduYunDownload\first&gt;
</code></pre>

<blockquote><p><code>basic</code> 基础
<code>crawl</code>自动爬虫
<code>csvfeed</code>用来处理csv文件
<code>xmlfeed</code>用来处理xml文件</p>

<p>按照<code>basic</code>模板创建一个名为<code>f1</code>的爬虫文件：<code>scrapy genspider -t basic f1</code> ，创建了一个<code>f1.py</code>文件。</p></blockquote>

<ul>
<li><p><strong>check</strong> 测试爬虫文件、或者说：检测一个爬虫，如果结果是：OK，那么说明结果没有问题。：<code>scrapy check f1</code></p></li>
<li><p><strong>crawl</strong> 运行一个爬虫文件。：<code>scrapy crawl f1</code> 或者 <code>scrapy crawl f1 --nolog</code></p></li>
<li><p><strong>list</strong> 列出当前爬虫项目下所有的爬虫文件： <code>scrapy list</code></p></li>
<li><p><strong>edit</strong> 使用编辑器打开爬虫文件 （Windows上似乎有问题，Linux上没有问题）：<code>scrapy edit f1</code></p></li>
</ul>


<hr />
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python3 大型网络爬虫实战 001 --- 搭建开发环境]]></title>
    <link href="http://aobojaing.github.io/blog/2016/11/26/python3-large-web-crawler-001-Build-development-environment/"/>
    <updated>2016-11-26T06:28:27+08:00</updated>
    <id>http://aobojaing.github.io/blog/2016/11/26/python3-large-web-crawler-001-Build-development-environment</id>
    <content type="html"><![CDATA[<hr />

<p>我使用的电脑： Windows 10 64位</p>

<h2>前言</h2>

<p>开发Python爬虫有很多种方式，从程序的复杂程度的角度来说，可以分为：爬虫项目和爬虫文件。
相信有些朋友玩过Python的urllib模块，一般我们可以用该模块写一些爬虫文件，实现起来非常方便，但做大型项目的时候，会发现效率不是太好、并且程序的稳定性也不是太好。
Scrapy是一个Python的爬虫框架，使用Scrapy可以提高开发效率，并且非常适合做一些中大型爬虫项目。
简单来说，urllib库更适合写爬虫文件，scrapy更适合做爬虫项目。</p>

<p>本套专栏，就来讲解如何做爬虫项目。本篇博客是第一篇博客：搭建开发环境。</p>

<h2>1 . 安装Python3</h2>

<p>到官网下载就可以了，下载一个Python3.5版本就可以，傻瓜式安装。</p>

<blockquote><p>Python 3 被默认安装在：<code>C:\Users\[Username]\AppData\Local\Programs\Python\Python35</code> 这个路径里面。</p></blockquote>

<h2>2 . 安装Python程序开发集成开发环境 &mdash; PyCharm IDE 2016.1.4</h2>

<p>软件下载：<a href="https://www.jetbrains.com/pycharm/download/#section=windows">https://www.jetbrains.com/pycharm/download/#section=windows</a></p>

<p>注意：</p>

<p>Professional是完整版的，但是需要注册码</p>

<p>注册方法：<a href="http://blog.csdn.net/tianzhaixing2013/article/details/44997881">http://blog.csdn.net/tianzhaixing2013/article/details/44997881</a></p>

<p>我这次安装的是PyCharm 2016。</p>

<blockquote><p>Community是免费版的，但是软件里面的Terminal是不能使用的。</p></blockquote>

<h2>3 . 安装 Visual Studio 2015 软件</h2>

<p>要知道：为什么需要 Visual Studio 软件了。（参考<a href="https://blogs.msdn.microsoft.com/pythonengineering/2016/04/11/unable-to-find-vcvarsall-bat/">这个网站</a>）</p>

<p>如果不安装，当中你执行<code>pip install third-package-name</code>时，有时会出现下面这个错误：<code> error: Unable to find vcvarsall.bat</code></p>

<p><img src="/images/2016-11-26-python3-large-web-crawler-001-Build-development-environment/1480104934562.png" alt="Alt text" /></p>

<p>安装Visual Studio 2015 软件是为了安装里面的<strong>Python Tools 2.2.5 for Visual Studio 2015</strong>软件。</p>

<p><strong>下载和安装 Visual Studio 2015 软件 的方法在</strong><a href="http://www.aobosir.com/blog/2016/11/26/Python-pip-error-Unable-to-find-vcvarsall-bat/"><strong>这里</strong></a>。</p>

<h2>4 . 升级 pip 工具</h2>

<p>在DOS窗口中执行下面的命令来升级pip工具。</p>

<pre><code>python -m pip install --upgrade pip
</code></pre>

<h2>5 . 安装一些第三方库</h2>

<p>lxml、Twisted、pywin32、scrapy</p>

<p>lxml是一种可以迅速、灵活地处理 XML。
Twisted是用Python实现的基于事件驱动的网络引擎框架。
pywin32提供win32api。
Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。</p>

<hr />

<p>我们安装的是python3.5，并且我的电脑是64位的，所以：下载：</p>

<p>lxml‑3.6.4‑cp35‑cp35m‑win_amd64.whl</p>

<p>Twisted‑16.5.0‑cp35‑cp35m‑win_amd64.whl</p>

<p>pywin32‑220.1‑cp35‑cp35m‑win_amd64.whl</p>

<p>scrapy(直接使用命令：<code>pip.exe install scrapy</code> 来安装。)</p>

<hr />

<p>Python安装第三方库的方法：<a href="http://blog.csdn.net/github_35160620/article/details/52203682">http://blog.csdn.net/github_35160620/article/details/52203682</a></p>

<blockquote><p>注意：如果你的电脑之前安装了Python2，那么Python2 有自己的pip工具，Python3 也是有自己的pip工具，所以，如果你在DOS命令行上执行<code>pip install some-package-name</code>命令的时候，系统会使用哪个pip工具呢？是python2的pip，还是python3的pip？</p>

<p>这个问题，你可以在这篇博客里得到解决答案：<a href="http://www.aobosir.com/blog/2016/11/23/pip-install-python2-python3/">http://www.aobosir.com/blog/2016/11/23/pip-install-python2-python3/</a></p></blockquote>

<hr />

<p>下载后，在我的电脑上是这样安装：</p>

<p>安装 lxml：</p>

<pre><code>C:\Users\AOBO&gt;cd C:\Users\AOBO\AppData\Local\Programs\Python\Python35\Scripts
C:\Users\AOBO\AppData\Local\Programs\Python\Python35\Scripts&gt;pip.exe install D:\software_install_package_win\python\some-Python-third-packages\lxml-3.6.4-cp35-cp35m-win_amd64.whl
Processing d:\software_install_package_win\python\some-python-third-packages\lxml-3.6.4-cp35-cp35m-win_amd64.whl
Installing collected packages: lxml
Successfully installed lxml-3.6.4
</code></pre>

<p>安装 Twisted ：（执行到<code>Collecting constantly&gt;=15.1 (from Twisted==16.5.0)</code>这句时，卡住了，我按了 Ctrl+C 才继续执行下去。自动下载了下面的：constantly、incremental、zope.interface 这三个依赖库）</p>

<pre><code>C:\Users\AOBO\AppData\Local\Programs\Python\Python35\Scripts&gt;pip.exe install D:\software_install_package_win\python\some-Python-third-packages\Twisted-16.5.0-cp35-cp35m-win_amd64.whl
Processing d:\software_install_package_win\python\some-python-third-packages\twisted-16.5.0-cp35-cp35m-win_amd64.whl
Collecting constantly&gt;=15.1 (from Twisted==16.5.0)
#(执行到这卡住了，我按了 Ctrl+C 才继续执行下去。自动下载了下面的：constantly、incremental、zope.interface 这三个依赖库)
  Downloading constantly-15.1.0-py2.py3-none-any.whl
Collecting incremental&gt;=16.10.1 (from Twisted==16.5.0)
  Downloading incremental-16.10.1-py2.py3-none-any.whl
Collecting zope.interface&gt;=4.0.2 (from Twisted==16.5.0)
  Downloading zope.interface-4.3.2-cp35-cp35m-win_amd64.whl (136kB)
    100% |████████████████████████████████| 143kB 7.1kB/s
Requirement already satisfied: setuptools in c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages (from zope.interface&gt;=4.0.2-&gt;Twisted==16.5.0)
Installing collected packages: constantly, incremental, zope.interface, Twisted
Successfully installed Twisted-16.5.0 constantly-15.1.0 incremental-16.10.1 zope.interface-4.3.2
</code></pre>

<p>安装pywin32：</p>

<pre><code>C:\Users\AOBO\AppData\Local\Programs\Python\Python35\Scripts&gt;pip.exe install D:\software_install_package_win\python\some-Python-third-packages\pywin32-220.1-cp35-cp35m-win_amd64.whl
Processing d:\software_install_package_win\python\some-python-third-packages\pywin32-220.1-cp35-cp35m-win_amd64.whl
Installing collected packages: pywin32
Successfully installed pywin32-220.1
</code></pre>

<p>安装scropy：</p>

<pre><code>C:\Users\AOBO\AppData\Local\Programs\Python\Python35\Scripts&gt;pip.exe install scrapy
Collecting scrapy
  Downloading Scrapy-1.2.1-py2.py3-none-any.whl (294kB)
    100% |████████████████████████████████| 296kB 338kB/s
Collecting service-identity (from scrapy)
  Downloading service_identity-16.0.0-py2.py3-none-any.whl
Collecting six&gt;=1.5.2 (from scrapy)
  Downloading six-1.10.0-py2.py3-none-any.whl
Collecting w3lib&gt;=1.15.0 (from scrapy)
  Downloading w3lib-1.16.0-py2.py3-none-any.whl
Collecting PyDispatcher&gt;=2.0.5 (from scrapy)
  Downloading PyDispatcher-2.0.5.tar.gz
Requirement already satisfied: Twisted&gt;=10.0.0 in c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages (from scrapy)
Requirement already satisfied: lxml in c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages (from scrapy)
Collecting cssselect&gt;=0.9 (from scrapy)
  Downloading cssselect-1.0.0-py2.py3-none-any.whl
Collecting parsel&gt;=0.9.3 (from scrapy)
  Downloading parsel-1.1.0-py2.py3-none-any.whl
Collecting queuelib (from scrapy)
  Downloading queuelib-1.4.2-py2.py3-none-any.whl
Collecting pyOpenSSL (from scrapy)
  Downloading pyOpenSSL-16.2.0-py2.py3-none-any.whl (43kB)
    100% |████████████████████████████████| 51kB 4.7MB/s
Collecting pyasn1 (from service-identity-&gt;scrapy)
  Downloading pyasn1-0.1.9-py2.py3-none-any.whl
Collecting pyasn1-modules (from service-identity-&gt;scrapy)
  Downloading pyasn1_modules-0.0.8-py2.py3-none-any.whl
Collecting attrs (from service-identity-&gt;scrapy)
  Downloading attrs-16.2.0-py2.py3-none-any.whl
Requirement already satisfied: constantly&gt;=15.1 in c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages (from Twisted&gt;=10.0.0-&gt;scrapy)
Requirement already satisfied: zope.interface&gt;=4.0.2 in c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages (from Twisted&gt;=10.0.0-&gt;scrapy)
Requirement already satisfied: incremental&gt;=16.10.1 in c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages (from Twisted&gt;=10.0.0-&gt;scrapy)
Collecting cryptography&gt;=1.3.4 (from pyOpenSSL-&gt;scrapy)
  Downloading cryptography-1.6-cp35-cp35m-win_amd64.whl (1.3MB)
    100% |████████████████████████████████| 1.3MB 257kB/s
Requirement already satisfied: setuptools in c:\users\aobo\appdata\local\programs\python\python35\lib\site-packages (from zope.interface&gt;=4.0.2-&gt;Twisted&gt;=10.0.0-&gt;scrapy)
Collecting cffi&gt;=1.4.1 (from cryptography&gt;=1.3.4-&gt;pyOpenSSL-&gt;scrapy)
  Downloading cffi-1.9.1-cp35-cp35m-win_amd64.whl (158kB)
    100% |████████████████████████████████| 163kB 322kB/s
Collecting idna&gt;=2.0 (from cryptography&gt;=1.3.4-&gt;pyOpenSSL-&gt;scrapy)
  Downloading idna-2.1-py2.py3-none-any.whl (54kB)
    100% |████████████████████████████████| 61kB 4.4MB/s
Collecting pycparser (from cffi&gt;=1.4.1-&gt;cryptography&gt;=1.3.4-&gt;pyOpenSSL-&gt;scrapy)
  Downloading pycparser-2.17.tar.gz (231kB)
    100% |████████████████████████████████| 235kB 311kB/s
Installing collected packages: six, pycparser, cffi, pyasn1, idna, cryptography, pyOpenSSL, pyasn1-modules, attrs, service-identity, w3lib, PyDispatcher, cssselect, parsel, queuelib, scrapy
  Running setup.py install for pycparser ... done
  Running setup.py install for PyDispatcher ... done
Successfully installed PyDispatcher-2.0.5 attrs-16.2.0 cffi-1.9.1 cryptography-1.6 cssselect-1.0.0 idna-2.1 parsel-1.1.0 pyOpenSSL-16.2.0 pyasn1-0.1.9 pyasn1-modules-0.0.8 pycparser-2.17 queuelib-1.4.2 scrapy-1.2.1 service-identity-16.0.0 six-1.10.0 w3lib-1.16.0
</code></pre>

<hr />

<p>查看 <code>scrapy</code> 是否安装成功：（执行<code>scrapy -h</code> 命令，如果能输出信息，说明安装成功）</p>

<pre><code>C:\Users\AOBO&gt;scrapy -h
Scrapy 1.2.1 - no active project

Usage:
  scrapy &lt;command&gt; [options] [args]

Available commands:
  bench         Run quick benchmark test
  commands
  fetch         Fetch a URL using the Scrapy downloader
  genspider     Generate new spider using pre-defined templates
  runspider     Run a self-contained spider (without creating a project)
  settings      Get settings values
  shell         Interactive scraping console
  startproject  Create new project
  version       Print Scrapy version
  view          Open URL in browser, as seen by Scrapy

  [ more ]      More commands available when run from project directory

Use "scrapy &lt;command&gt; -h" to see more info about a command

C:\Users\AOBO&gt;
</code></pre>

<hr />

<p>检查所有刚刚安装的库是否安装成功：</p>

<p>启动<strong>PyCharm</strong> 软件，新建一个工程：</p>

<p><img src="/images/2016-11-26-python3-large-web-crawler-001-Build-development-environment/1479835108332.png" alt="Alt text" /></p>

<p><img src="/images/2016-11-26-python3-large-web-crawler-001-Build-development-environment/1479835159526.png" alt="Alt text" /></p>

<p>刚刚安装的库在这里可以看到：</p>

<p><img src="/images/2016-11-26-python3-large-web-crawler-001-Build-development-environment/1479840214905.png" alt="Alt text" /></p>

<p>安装成功。</p>

<hr />

<h2>6 . 一个超好的命令行串口软件 &mdash; PowerCmd</h2>

<p>PowerCmd 是一款Windows CMD 的增强工具。</p>

<p>下载安装地址：<a href="http://www.aobosir.com/blog/2016/11/23/powercmd-install/">http://www.aobosir.com/blog/2016/11/23/powercmd-install/</a></p>

<blockquote><p>这个软件真的很喽，像我执行<code>scrapy -h</code> 这样的命令，都打印不出信息，在DOS窗口里面是有信息打印出来的。</p></blockquote>

<hr />

<hr />

<h2>测试环境</h2>

<p>1 . 执行 <code>scrapy -h</code>，如果有打印出来信息，说明Scrapy  安装成功。</p>

<p>2 . 执行 <code>scrapy bench</code> ，如果遇到问题，说明pywin32库还有需要完成的步骤。（解决问题:  import win32api ImportError: DLL load failed，到这里查看解决办法。）</p>

<hr />

<p>接下来，我们学习 Scrapy 的命令。了解了<strong>Scrapy</strong> 命令后，我学习：scrapy 爬虫项目的创建及爬虫的创建 &mdash; 实例：爬取百度标题和CSDN博客。</p>

<hr />
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python3 Pip 解决问题： Error: Unable to Find vcvarsall.bat]]></title>
    <link href="http://aobojaing.github.io/blog/2016/11/26/Python-pip-error-Unable-to-find-vcvarsall-bat/"/>
    <updated>2016-11-26T05:50:14+08:00</updated>
    <id>http://aobojaing.github.io/blog/2016/11/26/Python-pip-error-Unable-to-find-vcvarsall-bat</id>
    <content type="html"><![CDATA[<hr />

<p>当我给 <strong>python3.5</strong> 安装 第三方库 <code>charset</code> 时：<code>pip install charset</code>，出现了错误：</p>

<p><img src="/images/2016-11-26-Python-pip-error-Unable-to-find-vcvarsall-bat/1480092727015.png" alt="Alt text" /></p>

<pre><code>D:\WorkSpace\python_ws\python-large-web-crawler\firstdemo&gt;pip install charset
Collecting charset
  Downloading charset-1.0.1.tar.gz (189kB)
    100% |████████████████████████████████| 194kB 3.9kB/s
Collecting chardet (from charset)
  Using cached chardet-2.3.0.tar.gz
Installing collected packages: chardet, charset
  Running setup.py install for chardet ... done
  Running setup.py install for charset ... error
    Complete output from command c:\users\aobo\appdata\local\programs\python\python35\python.exe -u -c "import setuptools, tokenize;__file__='C:\\Users\\AOBO\\AppData\\Local\\Temp\\pip-build-ydv8oep3\\charset\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))" install --record C:\Users\AOBO\AppData\Local\Temp\pip-hlxpja30-record\install-record.txt --single-version-externally-managed --compile:
    running install
    running build
    running build_py
    creating build
    creating build\lib.win-amd64-3.5
    creating build\lib.win-amd64-3.5\charset
    copying charset\cmd.py -&gt; build\lib.win-amd64-3.5\charset
    copying charset\__init__.py -&gt; build\lib.win-amd64-3.5\charset
    running egg_info
    writing charset.egg-info\PKG-INFO
    writing top-level names to charset.egg-info\top_level.txt
    writing dependency_links to charset.egg-info\dependency_links.txt
    writing requirements to charset.egg-info\requires.txt
    writing entry points to charset.egg-info\entry_points.txt
    warning: manifest_maker: standard file '-c' not found

    reading manifest file 'charset.egg-info\SOURCES.txt'
    reading manifest template 'MANIFEST.in'
    warning: no files found matching '*.txt' under directory 'docs'
    warning: no files found matching '*.txt' under directory 'languagedet\data'
    warning: no files found matching '*.pickle' under directory 'languagedet\data'
    warning: no files found matching '*.conf' under directory 'languagedet\data'
    writing manifest file 'charset.egg-info\SOURCES.txt'
    running build_ext
    building 'charset.detector' extension
    error: Unable to find vcvarsall.bat

    ----------------------------------------
Command "c:\users\aobo\appdata\local\programs\python\python35\python.exe -u -c "import setuptools, tokenize;__file__='C:\\Users\\AOBO\\AppData\\Local\\Temp\\pip-build-ydv8oep3\\charset\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))" install --record C:\Users\AOBO\AppData\Local\Temp\pip-hlxpja30-record\install-record.txt --single-version-externally-managed --compile" failed with error code 1 in C:\Users\AOBO\AppData\Local\Temp\pip-build-ydv8oep3\charset\
</code></pre>

<hr />

<h2>为什么出现这个问题？</h2>

<p>在命令行中执行：python，看看当前使用的python的版本，和它所需要的Vs软件的编译器的版本：</p>

<p><img src="/images/2016-11-26-Python-pip-error-Unable-to-find-vcvarsall-bat/1480095090271.png" alt="Alt text" /></p>

<p>当前python版本是：python3.5.0；当前需要的Vs编译器的版本是：MSC v. 1900</p>

<p>查看下面的表格，对于版本的Visual C ++使用的编译器版本如下：（<a href="http://stackoverflow.com/questions/2676763/what-version-of-visual-studio-is-python-on-my-computer-compiled-with">表的参考网站</a>）</p>

<table>
<thead>
<tr>
<th style="text-align:left;"> Visual C ++版本 </th>
<th style="text-align:right;">     编译器版本</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> Visual C++ 4.x </td>
<td style="text-align:right;">   MSC_VER=1000 </td>
</tr>
<tr>
<td style="text-align:left;">Visual C++ 5                   </td>
<td style="text-align:right;"> MSC_VER=1100</td>
</tr>
<tr>
<td style="text-align:left;">Visual C++ 6                   </td>
<td style="text-align:right;"> MSC_VER=1200</td>
</tr>
<tr>
<td style="text-align:left;">Visual C++ .NET               </td>
<td style="text-align:right;">  MSC_VER=1300</td>
</tr>
<tr>
<td style="text-align:left;">Visual C++ .NET 2003         </td>
<td style="text-align:right;">   MSC_VER=1310</td>
</tr>
<tr>
<td style="text-align:left;">Visual C++ 2005  (8.0)      </td>
<td style="text-align:right;">    MSC_VER=1400</td>
</tr>
<tr>
<td style="text-align:left;">Visual C++ 2008  (9.0)     </td>
<td style="text-align:right;">     MSC_VER=1500</td>
</tr>
<tr>
<td style="text-align:left;">Visual C++ 2010 (10.0)    </td>
<td style="text-align:right;">      MSC_VER=1600</td>
</tr>
<tr>
<td style="text-align:left;">Visual C++ 2012 (11.0)   </td>
<td style="text-align:right;">       MSC_VER=1700</td>
</tr>
<tr>
<td style="text-align:left;">Visual C++ 2013 (12.0)  </td>
<td style="text-align:right;">        MSC_VER=1800</td>
</tr>
<tr>
<td style="text-align:left;">Visual C++ 2015 (14.0) </td>
<td style="text-align:right;">         MSC_VER=1900</td>
</tr>
</tbody>
</table>


<p>所以解决这个 <code>error: Unable to find vcvarsall.bat</code> 问题的方法就是：下载并安装 Visual Studio 2015 软件，问题即可解决。</p>

<h2>解决办法 &mdash; 安装：<strong>Python Tools 2.2.5 for Visual Studio 2015</strong></h2>

<p><strong>Step 1 . </strong> 下载 Visual Studio 2015 软件。</p>

<p>下载和安装 Visual Studio 2015 软件 的详细步骤请到这个博客查看：<a href="http://www.aobosir.com/blog/2016/11/26/download-install-Miarosoft-Visual-Studio-2015-software-tutorial/">下载和安装 Visual Studio 2015 软件 的详细步骤图文教程</a>。（<strong>我们按照这个网站的方法安装VS2015，但不按照这个博客里面说的安装。</strong>）</p>

<blockquote><p>如果安装上面的网站的方法安装VS2015软件，那么问题还是不能解决。（<code>error: Unable to find vcvarsall.bat</code>）</p></blockquote>

<p><strong>Step 2 . </strong> 安装 Visual Studio 2015 软件</p>

<p>这个VS2015，安装时需要选择：<strong>自定义安装</strong>。（<a href="http://jingyan.baidu.com/article/adc815138162e8f723bf7387.html">参考网站</a>）</p>

<p>参考网站：</p>

<p><img src="/images/2016-11-26-Python-pip-error-Unable-to-find-vcvarsall-bat/1480108534366.png" alt="Alt text" /></p>

<p><img src="/images/2016-11-26-Python-pip-error-Unable-to-find-vcvarsall-bat/1480108637845.png" alt="Alt text" /></p>

<p>我们的目的就是安装这个软件：<strong>Python Tools 2.2.5 for Visual Studio 2015</strong> 。现在，这个软件已经安装完了。</p>

<p><img src="/images/2016-11-26-Python-pip-error-Unable-to-find-vcvarsall-bat/1480109167911.png" alt="Alt text" /></p>

<blockquote><p><strong>注意：</strong></p>

<p> 如果一直停留在：“正在配置您的系统，这可能需要一些时间”</p>

<p> <img src="/images/2016-11-26-Python-pip-error-Unable-to-find-vcvarsall-bat/1480109561343.png" alt="Alt text" /></p>

<p>  解决：关掉VS的所有进程。</p></blockquote>

<h2>搞定，问题解决</h2>

<hr />

<p>现在再执行：<code>pip install charset</code>。问题解决。</p>

<p><img src="/images/2016-11-26-Python-pip-error-Unable-to-find-vcvarsall-bat/1480109745990.png" alt="Alt text" /></p>

<hr />
]]></content>
  </entry>
  
</feed>
