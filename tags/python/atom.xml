<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: python | AoboSir 博客]]></title>
  <link href="http://aobojaing.github.io/tags/python/atom.xml" rel="self"/>
  <link href="http://aobojaing.github.io/"/>
  <updated>2016-11-26T06:40:37+08:00</updated>
  <id>http://aobojaing.github.io/</id>
  <author>
    <name><![CDATA[Aobo Jaing]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Python --- Scrapy 命令]]></title>
    <link href="http://aobojaing.github.io/blog/2016/11/26/python-Scrapy-command/"/>
    <updated>2016-11-26T06:39:53+08:00</updated>
    <id>http://aobojaing.github.io/blog/2016/11/26/python-Scrapy-command</id>
    <content type="html"><![CDATA[<hr />

<p>Scrapy 命令 分为两种：<strong>全局命令</strong> 和 <strong>项目命令</strong>。</p>

<p>全局命令：在哪里都能使用。</p>

<p>项目命令：必须在爬虫项目里面才能使用。</p>

<h2>全局命令</h2>

<pre><code>C:\Users\AOBO&gt;scrapy -h
Scrapy 1.2.1 - no active project

Usage:
  scrapy &lt;command&gt; [options] [args]

Available commands:
  bench         Run quick benchmark test
  commands
  fetch         Fetch a URL using the Scrapy downloader
  genspider     Generate new spider using pre-defined templates
  runspider     Run a self-contained spider (without creating a project)
  settings      Get settings values
  shell         Interactive scraping console
  startproject  Create new project
  version       Print Scrapy version
  view          Open URL in browser, as seen by Scrapy

  [ more ]      More commands available when run from project directory

Use "scrapy &lt;command&gt; -h" to see more info about a command
</code></pre>

<ul>
<li><strong>startproject</strong>：创建一个爬虫项目：<code>scrapy startproject demo</code>（<code>demo</code> 创建的爬虫项目的名字）</li>
<li><strong>runspider</strong> 运用单独一个爬虫文件：<code>scrapy runspider abc.py</code></li>
<li><strong>veiw</strong> 下载一个网页的源代码，并在默认的文本编辑器中打开这个源代码：<code>scrapy view http://www.aobossir.com/</code></li>
<li><strong>shell</strong> 进入交互终端，用于爬虫的调试（如果你不调试，那么就不常用）：<code>scrapy shell http://www.baidu.com --nolog</code>（<code>--nolog</code> 不显示日志信息）</li>
<li><strong>version</strong> 查看版本：（<code>scrapy version</code>）</li>
<li><strong>bench</strong> 测试本地硬件性能（工作原理：）：<code>scrapy bench</code> （如果遇到问题：解决问题:  <code>import win32api ImportError: DLL load failed</code>，到这里查看解决办法。）</li>
</ul>


<hr />

<h2>项目命令</h2>

<p>（进入项目路径，才能看到项目命令）</p>

<pre><code>D:\BaiduYunDownload\first&gt;scrapy -h
Scrapy 1.2.1 - project: first

Usage:
  scrapy &lt;command&gt; [options] [args]

Available commands:
  bench         Run quick benchmark test
  check         Check spider contracts
  commands
  crawl         Run a spider
  edit          Edit spider
  fetch         Fetch a URL using the Scrapy downloader
  genspider     Generate new spider using pre-defined templates
  list          List available spiders
  parse         Parse URL (using its spider) and print the results
  runspider     Run a self-contained spider (without creating a project)
  settings      Get settings values
  shell         Interactive scraping console
  startproject  Create new project
  version       Print Scrapy version
  view          Open URL in browser, as seen by Scrapy

Use "scrapy &lt;command&gt; -h" to see more info about a command

D:\BaiduYunDownload\first&gt;
</code></pre>

<ul>
<li><strong>genspider</strong> 创建一个爬虫文件，我们在爬虫项目里面才能创建爬虫文件（这个命令用的非常多）（<strong>startproject</strong>：创建一个爬虫项目）。创建爬虫文件是按照以下模板来创建的，使用<code>scrapy genspider -l</code> 命令查看有哪些模板。</li>
</ul>


<pre><code>D:\BaiduYunDownload\first&gt;scrapy genspider -l
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed

D:\BaiduYunDownload\first&gt;
</code></pre>

<blockquote><p><code>basic</code> 基础
<code>crawl</code>自动爬虫
<code>csvfeed</code>用来处理csv文件
<code>xmlfeed</code>用来处理xml文件</p>

<p>按照<code>basic</code>模板创建一个名为<code>f1</code>的爬虫文件：<code>scrapy genspider -t basic f1</code> ，创建了一个<code>f1.py</code>文件。</p></blockquote>

<ul>
<li><p><strong>check</strong> 测试爬虫文件、或者说：检测一个爬虫，如果结果是：OK，那么说明结果没有问题。：<code>scrapy check f1</code></p></li>
<li><p><strong>crawl</strong> 运行一个爬虫文件。：<code>scrapy crawl f1</code> 或者 <code>scrapy crawl f1 --nolog</code></p></li>
<li><p><strong>list</strong> 列出当前爬虫项目下所有的爬虫文件： <code>scrapy list</code></p></li>
<li><p><strong>edit</strong> 使用编辑器打开爬虫文件 （Windows上似乎有问题，Linux上没有问题）：<code>scrapy edit f1</code></p></li>
</ul>


<hr />
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Learning Python 013 按行读取文件（逐行读取） --- 按行写入文件（逐行写入） --- 实战：从字幕文件中提取字幕内容]]></title>
    <link href="http://aobojaing.github.io/blog/2016/11/18/Learning-python-013-read-and-write-one-line/"/>
    <updated>2016-11-18T04:41:50+08:00</updated>
    <id>http://aobojaing.github.io/blog/2016/11/18/Learning-python-013-read-and-write-one-line</id>
    <content type="html"><![CDATA[<hr />

<p>使用的开发集成环境：PyCharm 2016.1.4
使用的Python的版本：python 2.7.10</p>

<hr />

<h2>知识点：Python 按行读取文件</h2>

<h3>读取整个文件的内容</h3>

<pre><code class="python">f = open('filename.txt', 'r')
text = f.read()
f.close()

print text
</code></pre>

<h3>读取一行的内容（按行读文件的内容）</h3>

<p>参考网站：</p>

<p>Python逐行读取文件内容
<a href="http://www.cnblogs.com/sysuoyj/archive/2012/03/14/2395789.html">http://www.cnblogs.com/sysuoyj/archive/2012/03/14/2395789.html</a></p>

<pre><code class="python">f = open('filename.txt', 'r')

while 1:
    line = f.readline()
    if not line:
        break
    print text
    pass

f.close()
</code></pre>

<p>缺点：它依赖于前后文的关系，所以不能获取指定行的内容。</p>

<h3>读取一行的内容 和 行号</h3>

<p>参考网站：</p>

<p>python读取文件同时输出行号和内容
<a href="http://outofmemory.cn/code-snippet/3222/python-duqu-file-tongshi-output-xinghao-content">http://outofmemory.cn/code-snippet/3222/python-duqu-file-tongshi-output-xinghao-content</a></p>

<pre><code class="python">f = open('filename.txt', 'r')
for (num,value) in enumerate(f):
    print "line number", num, "is:", value
f.close()
</code></pre>

<p>缺点：虽然这段代码可以获取到指定行的内容，但是，使用<code>enumerate()</code>函数获取指定行内容的代价是：需要对所有<strong>行</strong>依次进行编号，可想运算量之大。</p>

<h3>读取指定行的内容</h3>

<p>参考网站：</p>

<p>Python 从指定行读取数据
<a href="http://blog.163.com/xiaowei_090513/blog/static/11771835920140251257802">http://blog.163.com/xiaowei_090513/blog/static/11771835920140251257802</a></p>

<pre><code class="python">import linecache

num = 20
linecache.getline('filename.txt', num)
</code></pre>

<p>优点：读取的是缓存内的数据，速度快，并且代码简单。</p>

<hr />

<h2>知识点：Python 按行写入文件</h2>

<p>参考网站：</p>

<p>Python文件读写
<a href="http://cxymrzero.github.io/blog/2015/03/19/python-file/">http://cxymrzero.github.io/blog/2015/03/19/python-file/</a>
Python读写文件
<a href="http://blog.csdn.net/adupt/article/details/4435615">http://blog.csdn.net/adupt/article/details/4435615</a></p>

<h3>写一行（新建文件、替换现有文件）</h3>

<pre><code class="python">f = open('namefile.txt', 'w')
f.write('the first line: hello world in the new file.\n')
f.write('the sencond line: ')
f.write('this is also the second line.\n')
f.close()

f = open('namefile.txt', 'r')
text = f.read()
f.close()
print text
</code></pre>

<p>执行输出：</p>

<pre><code>the first line: hello world in the new file.
the sencond line: this is also the second line.
</code></pre>

<p><code>f = open('namefile.txt', 'w')</code>这段代码的执行：如果有这个<code>namefile.txt</code>文件，那就将现有的<code>namefile.txt</code>文件里面的内容全部自动清空；如果没有这个<code>namefile.txt</code>文件，那就自动新建一个<code>namefile.txt</code>文件。</p>

<h3>写一行（在原有文件后面追加写入）</h3>

<pre><code class="python">f = open('namefile.txt', 'w+')
f.write('the last line: hello world in the new file.\n')
f.close()

f = open('namefile.txt', 'r')
text = f.read()
f.close()
print text
</code></pre>

<p>输出：</p>

<pre><code>the first line: hello world in the new file.
the sencond line: this is also the second line.
the last line: hello world in the new file.
</code></pre>

<hr />

<h2>实战：从字幕文件中提取字幕内容</h2>

<p>github源代码网址：<a href="https://github.com/AoboJaing/youtube-srt-to-txt">https://github.com/AoboJaing/youtube-srt-to-txt</a></p>

<h3>如何获取字幕文件</h3>

<p>参考网站：<a href="https://www.youtube.com/watch?v=d9ctCc2AXCw">https://www.youtube.com/watch?v=d9ctCc2AXCw</a></p>

<p>视频网站： <a href="https://www.youtube.com/">https://www.youtube.com/</a></p>

<p>字幕提取网站：<a href="http://mo.dbxdb.com/setting.html">http://mo.dbxdb.com/setting.html</a></p>

<h3>设计思路</h3>

<p>输入是英文字幕文件和中文字幕文件，输出是英文配中文的txt文件</p>

<p>英文字幕文件</p>

<p><img src="/images/2016-11-18-Learning-python-013-read-and-write-one-line/1479414135007.png" alt="Alt text" /></p>

<p>中文字幕文件</p>

<p><img src="/images/2016-11-18-Learning-python-013-read-and-write-one-line/1479414161811.png" alt="Alt text" /></p>

<p>输出：英文配中文的txt文件</p>

<p><img src="/images/2016-11-18-Learning-python-013-read-and-write-one-line/1479414198952.png" alt="Alt text" /></p>

<h3>代码</h3>

<pre><code class="python"># coding : utf-8

import linecache

file_srt = open('Servos - working principle and homemade types.srt', 'r')
file_txt = open('newfile.txt', 'w')

cout=1
for (num, value) in enumerate(file_srt):
    if cout == 5:
        cout = 1
    if cout == 3:
        file_txt.write(linecache.getline('en-Servos - working principle and homemade types.srt', num+1))
        file_txt.write(value)
    cout += 1

file_srt.close()
file_txt.close()
</code></pre>

<h3>运行输出</h3>

<pre><code>Youtube subtitles download by mo.dbxdb.com 
Youtube subtitles download by mo.dbxdb.com 
In this video I would like to explain the functionality of servos and how to convert conventional DC motors into homebuilt servos.
在这部影片中，我想解释一下舵机以及如何传统的直流电动机转换成自制伺服系统的功能。 
A servo is a device that produces motion accordant to a command signal from a control system.
伺服是，从一个控制系统产生运动一致来的命令信号的装置。 
Usually an electric motor is used to create a mechanical force and the servomechanism rotates at a velocity that approximates the command signal.
一般的电动机被用来创建一个机械力和伺服机构在近似于指令信号的速度旋转。 
...
...
</code></pre>
]]></content>
  </entry>
  
</feed>
